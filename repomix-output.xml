This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  workflows/
    publish-helm-chart.yaml
    publish-image.yaml
    release-drafter.yml
  release-drafter.yml
adr/
  0001_rule_outputs.md
api/
  openslo/
    v1/
      alertcondition_types.go
      alertnotificationtarget_types.go
      alertpolicy_types.go
      common_types.go
      datasource_types.go
      groupversion_info.go
      service_types.go
      sli_types.go
      slo_types.go
      zz_generated.deepcopy.go
  osko/
    v1alpha1/
      alertmanagerconfig_types.go
      common_types.go
      connection_details.go
      cortex_types.go
      groupversion_info.go
      mimir_types.go
      mimirrule_types.go
      zz_generated.deepcopy.go
cmd/
  main.go
config/
  default/
    kustomization.yaml
    manager_auth_proxy_patch.yaml
    manager_config_patch.yaml
  manager/
    kustomization.yaml
    manager.yaml
  prometheus/
    kustomization.yaml
    monitor.yaml
  rbac/
    alertcondition_editor_role.yaml
    alertcondition_viewer_role.yaml
    alertnotificationtarget_editor_role.yaml
    alertnotificationtarget_viewer_role.yaml
    alertpolicy_editor_role.yaml
    alertpolicy_viewer_role.yaml
    auth_proxy_client_clusterrole.yaml
    auth_proxy_role_binding.yaml
    auth_proxy_role.yaml
    auth_proxy_service.yaml
    datasource_editor_role.yaml
    datasource_viewer_role.yaml
    kustomization.yaml
    leader_election_role_binding.yaml
    leader_election_role.yaml
    osko_alertmanagerconfig_editor_role.yaml
    osko_alertmanagerconfig_viewer_role.yaml
    osko_mimirrule_editor_role.yaml
    osko_mimirrule_viewer_role.yaml
    role_binding.yaml
    role.yaml
    service_account.yaml
    service_editor_role.yaml
    service_viewer_role.yaml
    sli_editor_role.yaml
    sli_viewer_role.yaml
    slo_editor_role.yaml
    slo_viewer_role.yaml
    slo-kubernetes-operator_slo_editor_role.yaml
    slo-kubernetes-operator_slo_viewer_role.yaml
  samples/
    config_secret.yaml
    kustomization.yaml
    openslo_v1_datasource.yaml
    openslo_v1_slo.yaml
    osko_v1alpha1_alertmanagerconfig.yaml
    slo_gatekeeper_webhook_response_time_0005.yaml
devel/
  grafana/
    deployment.yaml
    mimir-datasource.yaml
    osko-1.json
    osko-2.json
    service.yaml
  grafana-agent/
    configmap.yaml
    deployment.yaml
  metrics-generator/
    metrics-generator.go
  mimir/
    alertmanager-default-config.yaml
    configmap.yaml
    deployment.yaml
    service.yaml
  kustomization.yaml
docs/
  labels-and-annotations.md
helm/
  osko/
    charts/
      crds/
        Chart.yaml
    templates/
      tests/
        test-connection.yaml
      _helpers.tpl
      cluster-role-binding.yaml
      cluster-role.yaml
      deployment.yaml
      hpa.yaml
      leader_election_role_binding.yaml
      leader_election_role.yaml
      NOTES.txt
      prometheusrule_editor_role_binding.yaml
      prometheusrule_editor_role.yaml
      serviceaccount.yaml
    .helmignore
    Chart.lock
    Chart.yaml
    values.yaml
  osko-crds/
    charts/
      crds/
        Chart.yaml
    .helmignore
    Chart.yaml
internal/
  config/
    config.go
    types.go
    utils.go
  controller/
    monitoring.coreos.com/
      prometheusrule_controller.go
      suite_test.go
    openslo/
      alertcondition_controller.go
      alertnotificationtarget_controller.go
      alertpolicy_controller.go
      datasource_controller.go
      sli_controller.go
      slo_controller.go
      suite_test.go
    osko/
      alertmanagerconfig_controller_test.go
      alertmanagerconfig_controller.go
      mimirrule_controller.go
      suite_test.go
  helpers/
    mimirtool_helper.go
    prometheus_helper.go
  utils/
    common_utils.go
test/
  templates/
    tests/
      test-connection.yaml
    _helpers.tpl
    deployment.yaml
    hpa.yaml
    ingress.yaml
    NOTES.txt
    service.yaml
    serviceaccount.yaml
  .helmignore
  Chart.yaml
  values.yaml
.dockerignore
.gitignore
.repomixignore
ct.yaml
DCO
DESIGN.md
Dockerfile
entitlements.xml
go.mod
LICENSE
Makefile
PROJECT
README.md
repomix.config.json
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="adr/0001_rule_outputs.md">
# Output metric naming

* Status: proposed
* Date: 2023-10-27

## Context and Problem Statement

The solution provided by the operator doesn't end with a `PrometheusRule` CRD, but takes into account as much of the SLO journey as possible. 
Part of that journey is displaying Indicators, Objectives, Error Budgets etc. on different dashboards. The design should account for
existence of reusable Grafana dashboards at the very least.

## Considered Options

* Prometheus Recording Rule naming convention
  * With part of slo in name: `slo:logging_query_frontend_error_rate:error_budget:28d`
  * With osko in name: `osko:error_budget{slo_name="logging_query_frontend_error_rate", window="28d", ...}`
* Pretend to be a metric
  * `osko_error_budget{slo_name="logging_query_frontend_error_rate", window="28d", ...}`

## Decision Outcome

Chosen option: "Pretend to be a metric". There is a reasonable chance that `PrometheusRule` CRD might not be the only possible output
of OSKO and thus it's worth it to avoid the result being too implementation specific.

The metric names that should be exposed/recorded:

* `osko_sli_ratio_good`
* `osko_sli_ratio_bad`
* `osko_sli_ratio_total`
* `osko_sli_measurement`
* `osko_slo_target`
* `osko_error_budget_available`

Ideally also the following, although these might be implementation specific (aka we'll see how hard it is):

* `osko_error_budget_burn_rate`
* `osko_error_budget_burn_rate_threshold`

The following labels should be present next to the metric (where applicable):

*Note that the following might change as we explore implementing composite SLOs and other more complex scenarios.*

* `sli_name`
* `slo_name`
* `service`
* `window`

### Positive Consequences

* Future implementations will be able to plug in to the rest of the solution
* Exhaustive list of metrics will make creation of the dashboards easy

### Negative Consequences

* Tracing resulting metric back to the original will be only possible by looking at the SLI specification manifest.

## Pros and Cons of the Options

### Prometheus Recording Rule naming convention - With part of slo in name

`slo:loki_request_duration_seconds_count:ratio_good:28d`

* Good, because it can show the original metric
* Bad, because dashboards would have to be generated for each metric

### Prometheus Recording Rule naming convention -  With osko in name

`osko:error_budget{slo_name="logging_query_frontend_error_rate", window="28d", ...}`

* Good, because it allows for reusable dashboards
* Bad, because it's tightly tied to Prometheus recording rule implementation

## Links 

* [Prometheus recordingrule naming recommendations](https://prometheus.io/docs/practices/rules/#naming)
</file>

<file path="api/openslo/v1/alertcondition_types.go">
package v1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
type ConditionSpec struct {
	// +kubebuilder:validation:Enum=Burnrate
	Kind string `json:"kind,omitempty"`
	// +kubebuilder:validation:Enum=lte;gte;lt;gt
	Op             string   `json:"op,omitempty"`
	Threshold      string   `json:"threshold,omitempty"`
	LookbackWindow Duration `json:"lookbackWindow,omitempty"`
	AlertAfter     Duration `json:"alertAfter,omitempty"`
}
⋮----
// +kubebuilder:validation:Enum=Burnrate
⋮----
// +kubebuilder:validation:Enum=lte;gte;lt;gt
⋮----
// AlertConditionSpec defines the desired state of AlertCondition
type AlertConditionSpec struct {
	Description Description   `json:"description,omitempty"`
	Severity    string        `json:"severity,omitempty"`
	Condition   ConditionSpec `json:"condition,omitempty"`
}
⋮----
// AlertConditionStatus defines the observed state of AlertCondition
type AlertConditionStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}
⋮----
// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
// Important: Run "make" to regenerate code after modifying this file
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
⋮----
// AlertCondition is the Schema for the alertconditions API
type AlertCondition struct {
	metav1.TypeMeta   `json:",inline"`
	ObjectMetaOpenSLO `json:"metadata,omitempty"`

	Spec   AlertConditionSpec   `json:"spec,omitempty"`
	Status AlertConditionStatus `json:"status,omitempty"`
}
⋮----
// AlertConditionList contains a list of AlertCondition
type AlertConditionList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []AlertCondition `json:"items"`
}
⋮----
func init()
</file>

<file path="api/openslo/v1/alertnotificationtarget_types.go">
package v1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
// AlertNotificationTargetSpec defines the desired state of AlertNotificationTarget
type AlertNotificationTargetSpec struct {
	Description Description `json:"description,omitempty"`
	Target      string      `json:"target,omitempty"`
}
⋮----
// AlertNotificationTargetStatus defines the observed state of AlertNotificationTarget
type AlertNotificationTargetStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}
⋮----
// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
// Important: Run "make" to regenerate code after modifying this file
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
⋮----
// AlertNotificationTarget is the Schema for the alertnotificationtargets API
type AlertNotificationTarget struct {
	metav1.TypeMeta   `json:",inline"`
	ObjectMetaOpenSLO `json:"metadata,omitempty"`

	Spec   AlertNotificationTargetSpec   `json:"spec,omitempty"`
	Status AlertNotificationTargetStatus `json:"status,omitempty"`
}
⋮----
// AlertNotificationTargetList contains a list of AlertNotificationTarget
type AlertNotificationTargetList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []AlertNotificationTarget `json:"items"`
}
⋮----
func init()
</file>

<file path="api/openslo/v1/alertpolicy_types.go">
package v1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
type AlertPolicyNotificationTarget struct {
	// +kubebuilder:validation:Enum=AlertNotificationTarget
	Kind      string                       `json:"kind,omitempty"`
	Metadata  metav1.ObjectMeta            `json:"metadata,omitempty"`
	Spec      *AlertNotificationTargetSpec `json:"spec,omitempty"`
	TargetRef *string                      `json:"targetRef,omitempty"`
}
⋮----
// +kubebuilder:validation:Enum=AlertNotificationTarget
⋮----
type AlertPolicyCondition struct {
	// +kubebuilder:validation:Enum=AlertCondition
	Kind         string              `json:"kind,omitempty"`
	Metadata     ObjectMetaOpenSLO   `json:"metadata,omitempty"`
	Spec         *AlertConditionSpec `json:"spec,omitempty"`
	ConditionRef *string             `json:"conditionRef,omitempty"`
}
⋮----
// +kubebuilder:validation:Enum=AlertCondition
⋮----
// AlertPolicySpec defines the desired state of AlertPolicy
type AlertPolicySpec struct {
	Description        Description `json:"description,omitempty"`
	AlertWhenNoData    bool        `json:"alertWhenNoData,omitempty"`
	AlertWhenResolved  bool        `json:"alertWhenResolved,omitempty"`
	AlertWhenBreaching bool        `json:"alertWhenBreaching,omitempty"`
	// +kubebuilder:validation:MaxItems=1
	Conditions          []AlertPolicyCondition          `json:"conditions,omitempty"`
	NotificationTargets []AlertPolicyNotificationTarget `json:"notificationTargets,omitempty"`
}
⋮----
// +kubebuilder:validation:MaxItems=1
⋮----
// AlertPolicyStatus defines the observed state of AlertPolicy
type AlertPolicyStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}
⋮----
// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
// Important: Run "make" to regenerate code after modifying this file
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
⋮----
// AlertPolicy is the Schema for the alertpolicies API
type AlertPolicy struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   AlertPolicySpec   `json:"spec,omitempty"`
	Status AlertPolicyStatus `json:"status,omitempty"`
}
⋮----
// AlertPolicyList contains a list of AlertPolicy
type AlertPolicyList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []AlertPolicy `json:"items"`
}
⋮----
func init()
</file>

<file path="api/openslo/v1/datasource_types.go">
package v1
⋮----
import (
	osko "github.com/oskoperator/osko/api/osko/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
osko "github.com/oskoperator/osko/api/osko/v1alpha1"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
// ConnectionDetails specify how to connect to your metrics data provider
// +kubebuilder:validation:MinProperties=1
// +kubebuilder:validation:MaxProperties=1
type ConnectionDetails struct {
	Mimir  *osko.Mimir  `json:"mimir,omitempty"`
	Cortex *osko.Cortex `json:"cortex,omitempty"`
}
⋮----
// DatasourceSpec defines the desired state of Datasource
type DatasourceSpec struct {
	Description       Description            `json:"description,omitempty"`
	Type              string                 `json:"type,omitempty"`
	ConnectionDetails osko.ConnectionDetails `json:"connectionDetails,omitempty"`
}
⋮----
// DatasourceStatus defines the observed state of Datasource
type DatasourceStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}
⋮----
// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
// Important: Run "make" to regenerate code after modifying this file
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:resource:scope=Namespaced
⋮----
// Datasource is the Schema for the datasources API
type Datasource struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   DatasourceSpec   `json:"spec,omitempty"`
	Status DatasourceStatus `json:"status,omitempty"`
}
⋮----
// DatasourceList contains a list of Datasource
type DatasourceList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Datasource `json:"items"`
}
⋮----
func init()
</file>

<file path="api/openslo/v1/groupversion_info.go">
// Package v1 contains API Schema definitions for the openslo v1 API group
// +kubebuilder:object:generate=true
// +groupName=openslo.com
package v1
⋮----
import (
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/scheme"
)
⋮----
"k8s.io/apimachinery/pkg/runtime/schema"
"sigs.k8s.io/controller-runtime/pkg/scheme"
⋮----
var (
	// GroupVersion is group version used to register these objects
	GroupVersion = schema.GroupVersion{Group: "openslo.com", Version: "v1"}

	// SchemeBuilder is used to add go types to the GroupVersionKind scheme
	SchemeBuilder = &scheme.Builder{GroupVersion: GroupVersion}

	// AddToScheme adds the types in this group-version to the given scheme.
	AddToScheme = SchemeBuilder.AddToScheme
)
⋮----
// GroupVersion is group version used to register these objects
⋮----
// SchemeBuilder is used to add go types to the GroupVersionKind scheme
⋮----
// AddToScheme adds the types in this group-version to the given scheme.
</file>

<file path="api/openslo/v1/service_types.go">
package v1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
// ServiceSpec defines the desired state of Service
type ServiceSpec struct {
	Description Description `json:"description,omitempty"`
}
⋮----
type ServiceStatus struct{}
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
⋮----
// Service is the Schema for the services API
type Service struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   ServiceSpec   `json:"spec,omitempty"`
	Status ServiceStatus `json:"status,omitempty"`
}
⋮----
// ServiceList contains a list of Service
type ServiceList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []Service `json:"items"`
}
⋮----
func init()
</file>

<file path="api/openslo/v1/slo_types.go">
package v1
⋮----
import (
	"k8s.io/apimachinery/pkg/api/resource"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
"k8s.io/apimachinery/pkg/api/resource"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
type SLOAlertPolicy struct {
	// +kubebuilder:validation:Enum=AlertPolicy
	Kind string `json:"kind,omitempty"`
	// +kubebuilder:crd:generateEmbeddedObjectMeta=true
	Metadata       metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec           *AlertPolicySpec  `json:"spec,omitempty"`
	AlertPolicyRef *string           `json:"alertPolicyRef,omitempty"`
}
⋮----
// +kubebuilder:validation:Enum=AlertPolicy
⋮----
// +kubebuilder:crd:generateEmbeddedObjectMeta=true
⋮----
type Indicator struct {
	Metadata metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec     SLISpec           `json:"spec,omitempty"`
}
⋮----
type ObjectivesSpec struct {
	// +optional
	DisplayName string `json:"displayName,omitempty"`
	// +kubebuilder:validation:Enum=lte;gte;lt;gt
	Op              string            `json:"op,omitempty"`
	Value           string            `json:"value,omitempty"`
	Target          string            `json:"target,omitempty"`
	TargetPercent   string            `json:"targetPercent,omitempty"`
	TimeSliceTarget string            `json:"timeSliceTarget,omitempty"`
	TimeSliceWindow Duration          `json:"timeSliceWindow,omitempty"`
	Indicator       *Indicator        `json:"indicator,omitempty"`
	IndicatorRef    *string           `json:"indicatorRef,omitempty"`
	CompositeWeight resource.Quantity `json:"compositeWeight,omitempty"`
}
⋮----
// +optional
⋮----
// +kubebuilder:validation:Enum=lte;gte;lt;gt
⋮----
type CalendarSpec struct {
	// Date with time in 24h format, format without time zone
	// +kubebuilder:example="2020-01-21 12:30:00"
	StartTime string `json:"startTime,omitempty"`

	// Name as in IANA Time Zone Database
	// +kubebuilder:example="America/New_York"
	TimeZone string `json:"timeZone,omitempty"`
}
⋮----
// Date with time in 24h format, format without time zone
// +kubebuilder:example="2020-01-21 12:30:00"
⋮----
// Name as in IANA Time Zone Database
// +kubebuilder:example="America/New_York"
⋮----
type TimeWindowSpec struct {
	Duration  Duration     `json:"duration,omitempty"`
	IsRolling bool         `json:"isRolling,omitempty"`
	Calendar  CalendarSpec `json:"calendar,omitempty"`
}
⋮----
// SLOSpec defines the desired state of SLO
type SLOSpec struct {
	Description  Description `json:"description,omitempty"`
	Service      string      `json:"service,omitempty"`
	Indicator    *Indicator  `json:"indicator,omitempty"`
	IndicatorRef *string     `json:"indicatorRef,omitempty"`
	// +kubebuilder:validation:MaxItems=1
	TimeWindow []TimeWindowSpec `json:"timeWindow,omitempty"`
	// +kubebuilder:validation:Enum=Occurrences;Timeslices;RatioTimeslices
	BudgetingMethod string           `json:"budgetingMethod,omitempty"`
	Objectives      []ObjectivesSpec `json:"objectives,omitempty"`
	AlertPolicies   []SLOAlertPolicy `json:"alertPolicies,omitempty"`
}
⋮----
// +kubebuilder:validation:MaxItems=1
⋮----
// +kubebuilder:validation:Enum=Occurrences;Timeslices;RatioTimeslices
⋮----
// SLOStatus defines the observed state of SLO
type SLOStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
	Conditions         []metav1.Condition `json:"conditions,omitempty"`
	CurrentSLO         string             `json:"currentSLO,omitempty"`
	LastEvaluationTime metav1.Time        `json:"lastEvaluationTime,omitempty"`
	Ready              string             `json:"ready,omitempty"`
}
⋮----
// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
// Important: Run "make" to regenerate code after modifying this file
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:printcolumn:name="Ready",type=string,JSONPath=.status.ready,description="The reason for the current status of the SLO resource"
//+kubebuilder:printcolumn:name="Window",type=string,JSONPath=.spec.timeWindow[0].duration,description="The time window for the SLO resource"
//+kubebuilder:printcolumn:name="Age",type=date,JSONPath=.metadata.creationTimestamp,description="The time when the SLO resource was created"
⋮----
// SLO is the Schema for the slos API
type SLO struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`
	Spec              SLOSpec   `json:"spec,omitempty"`
	Status            SLOStatus `json:"status,omitempty"`
}
⋮----
// SLOList contains a list of SLO
type SLOList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []SLO `json:"items"`
}
⋮----
func init()
</file>

<file path="api/osko/v1alpha1/common_types.go">
package v1alpha1
⋮----
type Ruler struct {
	// +kubebuilder:default=false
	Enabled bool   `json:"enabled,omitempty"`
	Subpath string `json:"subpath,omitempty"`
}
⋮----
// +kubebuilder:default=false
⋮----
type Multitenancy struct {
	// +kubebuilder:default=false
	Enabled bool `json:"enabled,omitempty"`
	// +kubebuilder:MinItems=1
	SourceTenants []string `json:"sourceTenants,omitempty"`
	TargetTenant  string   `json:"targetTenant,omitempty"`
}
⋮----
// +kubebuilder:MinItems=1
</file>

<file path="api/osko/v1alpha1/connection_details.go">
package v1alpha1
⋮----
// +kubebuilder:object:generate=true
type ConnectionDetails struct {
	Address             string   `json:"address,omitempty"`
	TargetTenant        string   `json:"targetTenant,omitempty"`
	SourceTenants       []string `json:"sourceTenants,omitempty"`
	SyncPrometheusRules bool     `json:"syncPrometheusRules,omitempty"`
}
</file>

<file path="api/osko/v1alpha1/cortex_types.go">
package v1alpha1
⋮----
// +kubebuilder:object:generate=true
type Cortex struct {
	Address      string       `json:"address,omitempty"`
	Ruler        Ruler        `json:"ruler,omitempty"`
	Multitenancy Multitenancy `json:"multitenancy,omitempty"`
}
</file>

<file path="api/osko/v1alpha1/groupversion_info.go">
// Package v1alpha1 contains API Schema definitions for the slo-kubernetes-operator v1alpha1 API group
// +kubebuilder:object:generate=true
// +groupName=osko.dev
package v1alpha1
⋮----
import (
	"k8s.io/apimachinery/pkg/runtime/schema"
	"sigs.k8s.io/controller-runtime/pkg/scheme"
)
⋮----
"k8s.io/apimachinery/pkg/runtime/schema"
"sigs.k8s.io/controller-runtime/pkg/scheme"
⋮----
var (
	// GroupVersion is group version used to register these objects
	GroupVersion = schema.GroupVersion{Group: "osko.dev", Version: "v1alpha1"}

	// SchemeBuilder is used to add go types to the GroupVersionKind scheme
	SchemeBuilder = &scheme.Builder{GroupVersion: GroupVersion}

	// AddToScheme adds the types in this group-version to the given scheme.
	AddToScheme = SchemeBuilder.AddToScheme
)
⋮----
// GroupVersion is group version used to register these objects
⋮----
// SchemeBuilder is used to add go types to the GroupVersionKind scheme
⋮----
// AddToScheme adds the types in this group-version to the given scheme.
</file>

<file path="api/osko/v1alpha1/mimir_types.go">
package v1alpha1
⋮----
// +kubebuilder:object:generate=true
type Mimir struct {
	Address      string       `json:"address,omitempty"`
	Ruler        Ruler        `json:"ruler,omitempty"`
	Multitenancy Multitenancy `json:"multitenancy,omitempty"`
}
</file>

<file path="config/default/kustomization.yaml">
# Adds namespace to all resources.
namespace: slo-kubernetes-operator-system

# Value of this field is prepended to the
# names of all resources, e.g. a deployment named
# "wordpress" becomes "alices-wordpress".
# Note that it should also match with the prefix (text before '-') of the namespace
# field above.
namePrefix: slo-kubernetes-operator-

# Labels to add to all resources and selectors.
#labels:
#- includeSelectors: true
#  pairs:
#    someName: someValue

resources:
- ../crd
- ../rbac
- ../manager
# [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in
# crd/kustomization.yaml
#- ../webhook
# [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER'. 'WEBHOOK' components are required.
#- ../certmanager
# [PROMETHEUS] To enable prometheus monitor, uncomment all sections with 'PROMETHEUS'.
#- ../prometheus

patchesStrategicMerge:
# Protect the /metrics endpoint by putting it behind auth.
# If you want your controller-manager to expose the /metrics
# endpoint w/o any authn/z, please comment the following line.
- manager_auth_proxy_patch.yaml



# [WEBHOOK] To enable webhook, uncomment all the sections with [WEBHOOK] prefix including the one in
# crd/kustomization.yaml
#- manager_webhook_patch.yaml

# [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER'.
# Uncomment 'CERTMANAGER' sections in crd/kustomization.yaml to enable the CA injection in the admission webhooks.
# 'CERTMANAGER' needs to be enabled to use ca injection
#- webhookcainjection_patch.yaml

# [CERTMANAGER] To enable cert-manager, uncomment all sections with 'CERTMANAGER' prefix.
# Uncomment the following replacements to add the cert-manager CA injection annotations
#replacements:
#  - source: # Add cert-manager annotation to ValidatingWebhookConfiguration, MutatingWebhookConfiguration and CRDs
#      kind: Certificate
#      group: cert-manager.io
#      version: v1
#      name: serving-cert # this name should match the one in certificate.yaml
#      fieldPath: .metadata.namespace # namespace of the certificate CR
#    targets:
#      - select:
#          kind: ValidatingWebhookConfiguration
#        fieldPaths:
#          - .metadata.annotations.[cert-manager.io/inject-ca-from]
#        options:
#          delimiter: '/'
#          index: 0
#          create: true
#      - select:
#          kind: MutatingWebhookConfiguration
#        fieldPaths:
#          - .metadata.annotations.[cert-manager.io/inject-ca-from]
#        options:
#          delimiter: '/'
#          index: 0
#          create: true
#      - select:
#          kind: CustomResourceDefinition
#        fieldPaths:
#          - .metadata.annotations.[cert-manager.io/inject-ca-from]
#        options:
#          delimiter: '/'
#          index: 0
#          create: true
#  - source:
#      kind: Certificate
#      group: cert-manager.io
#      version: v1
#      name: serving-cert # this name should match the one in certificate.yaml
#      fieldPath: .metadata.name
#    targets:
#      - select:
#          kind: ValidatingWebhookConfiguration
#        fieldPaths:
#          - .metadata.annotations.[cert-manager.io/inject-ca-from]
#        options:
#          delimiter: '/'
#          index: 1
#          create: true
#      - select:
#          kind: MutatingWebhookConfiguration
#        fieldPaths:
#          - .metadata.annotations.[cert-manager.io/inject-ca-from]
#        options:
#          delimiter: '/'
#          index: 1
#          create: true
#      - select:
#          kind: CustomResourceDefinition
#        fieldPaths:
#          - .metadata.annotations.[cert-manager.io/inject-ca-from]
#        options:
#          delimiter: '/'
#          index: 1
#          create: true
#  - source: # Add cert-manager annotation to the webhook Service
#      kind: Service
#      version: v1
#      name: webhook-service
#      fieldPath: .metadata.name # namespace of the service
#    targets:
#      - select:
#          kind: Certificate
#          group: cert-manager.io
#          version: v1
#        fieldPaths:
#          - .spec.dnsNames.0
#          - .spec.dnsNames.1
#        options:
#          delimiter: '.'
#          index: 0
#          create: true
#  - source:
#      kind: Service
#      version: v1
#      name: webhook-service
#      fieldPath: .metadata.namespace # namespace of the service
#    targets:
#      - select:
#          kind: Certificate
#          group: cert-manager.io
#          version: v1
#        fieldPaths:
#          - .spec.dnsNames.0
#          - .spec.dnsNames.1
#        options:
#          delimiter: '.'
#          index: 1
#          create: true
</file>

<file path="config/default/manager_auth_proxy_patch.yaml">
# This patch inject a sidecar container which is a HTTP proxy for the
# controller manager, it performs RBAC authorization against the Kubernetes API using SubjectAccessReviews.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-manager
  namespace: system
spec:
  template:
    spec:
      containers:
      - name: kube-rbac-proxy
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - "ALL"
        image: gcr.io/kubebuilder/kube-rbac-proxy:v0.14.1
        args:
        - "--secure-listen-address=0.0.0.0:8443"
        - "--upstream=http://127.0.0.1:8080/"
        - "--logtostderr=true"
        - "--v=0"
        ports:
        - containerPort: 8443
          protocol: TCP
          name: https
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 5m
            memory: 64Mi
      - name: manager
        args:
        - "--health-probe-bind-address=:8081"
        - "--metrics-bind-address=127.0.0.1:8080"
        - "--leader-elect"
</file>

<file path="config/default/manager_config_patch.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-manager
  namespace: system
spec:
  template:
    spec:
      containers:
      - name: manager
</file>

<file path="config/manager/kustomization.yaml">
resources:
- manager.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
images:
- name: controller
  newName: localhost:5000/slo-kubernetes-operator/slo-kubernetes-operator/operator
  newTag: latest
</file>

<file path="config/manager/manager.yaml">
apiVersion: v1
kind: Namespace
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: namespace
    app.kubernetes.io/instance: system
    app.kubernetes.io/component: manager
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: system
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: controller-manager
  namespace: system
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: deployment
    app.kubernetes.io/instance: controller-manager
    app.kubernetes.io/component: manager
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
spec:
  selector:
    matchLabels:
      control-plane: controller-manager
  replicas: 1
  template:
    metadata:
      annotations:
        kubectl.kubernetes.io/default-container: manager
      labels:
        control-plane: controller-manager
    spec:
      securityContext:
        runAsNonRoot: true
      containers:
      - command:
        - /manager
        args:
        - --leader-elect
        image: controller:latest
        name: manager
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
              - "ALL"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
        resources:
          limits:
            cpu: 500m
            memory: 128Mi
          requests:
            cpu: 10m
            memory: 64Mi
      serviceAccountName: controller-manager
      terminationGracePeriodSeconds: 10
</file>

<file path="config/prometheus/kustomization.yaml">
resources:
- monitor.yaml
</file>

<file path="config/prometheus/monitor.yaml">
# Prometheus Monitor Service (Metrics)
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: servicemonitor
    app.kubernetes.io/instance: controller-manager-metrics-monitor
    app.kubernetes.io/component: metrics
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: controller-manager-metrics-monitor
  namespace: system
spec:
  endpoints:
    - path: /metrics
      port: https
      scheme: https
      bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
      tlsConfig:
        insecureSkipVerify: true
  selector:
    matchLabels:
      control-plane: controller-manager
</file>

<file path="config/rbac/alertcondition_editor_role.yaml">
# permissions for end users to edit alertconditions.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: alertcondition-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: alertcondition-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - alertconditions
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertconditions/status
  verbs:
  - get
</file>

<file path="config/rbac/alertcondition_viewer_role.yaml">
# permissions for end users to view alertconditions.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: alertcondition-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: alertcondition-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - alertconditions
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertconditions/status
  verbs:
  - get
</file>

<file path="config/rbac/alertnotificationtarget_editor_role.yaml">
# permissions for end users to edit alertnotificationtargets.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: alertnotificationtarget-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: alertnotificationtarget-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets/status
  verbs:
  - get
</file>

<file path="config/rbac/alertnotificationtarget_viewer_role.yaml">
# permissions for end users to view alertnotificationtargets.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: alertnotificationtarget-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: alertnotificationtarget-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets/status
  verbs:
  - get
</file>

<file path="config/rbac/alertpolicy_editor_role.yaml">
# permissions for end users to edit alertpolicies.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: alertpolicy-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: alertpolicy-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies/status
  verbs:
  - get
</file>

<file path="config/rbac/alertpolicy_viewer_role.yaml">
# permissions for end users to view alertpolicies.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: alertpolicy-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: alertpolicy-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies/status
  verbs:
  - get
</file>

<file path="config/rbac/auth_proxy_client_clusterrole.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: metrics-reader
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: metrics-reader
rules:
- nonResourceURLs:
  - "/metrics"
  verbs:
  - get
</file>

<file path="config/rbac/auth_proxy_role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/instance: proxy-rolebinding
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: proxy-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: proxy-role
subjects:
- kind: ServiceAccount
  name: controller-manager
  namespace: system
</file>

<file path="config/rbac/auth_proxy_role.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: proxy-role
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: proxy-role
rules:
- apiGroups:
  - authentication.k8s.io
  resources:
  - tokenreviews
  verbs:
  - create
- apiGroups:
  - authorization.k8s.io
  resources:
  - subjectaccessreviews
  verbs:
  - create
</file>

<file path="config/rbac/auth_proxy_service.yaml">
apiVersion: v1
kind: Service
metadata:
  labels:
    control-plane: controller-manager
    app.kubernetes.io/name: service
    app.kubernetes.io/instance: controller-manager-metrics-service
    app.kubernetes.io/component: kube-rbac-proxy
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: controller-manager-metrics-service
  namespace: system
spec:
  ports:
  - name: https
    port: 8443
    protocol: TCP
    targetPort: https
  selector:
    control-plane: controller-manager
</file>

<file path="config/rbac/datasource_editor_role.yaml">
# permissions for end users to edit datasources.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: datasource-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: datasource-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - datasources
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - datasources/status
  verbs:
  - get
</file>

<file path="config/rbac/datasource_viewer_role.yaml">
# permissions for end users to view datasources.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: datasource-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: datasource-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - datasources
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - datasources/status
  verbs:
  - get
</file>

<file path="config/rbac/leader_election_role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/name: rolebinding
    app.kubernetes.io/instance: leader-election-rolebinding
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: leader-election-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: leader-election-role
subjects:
- kind: ServiceAccount
  name: controller-manager
  namespace: system
</file>

<file path="config/rbac/leader_election_role.yaml">
# permissions to do leader election.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/name: role
    app.kubernetes.io/instance: leader-election-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: leader-election-role
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
</file>

<file path="config/rbac/role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/name: clusterrolebinding
    app.kubernetes.io/instance: manager-rolebinding
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: manager-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: manager-role
subjects:
- kind: ServiceAccount
  name: controller-manager
  namespace: system
</file>

<file path="config/rbac/service_account.yaml">
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/name: serviceaccount
    app.kubernetes.io/instance: controller-manager-sa
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: controller-manager
  namespace: system
</file>

<file path="config/rbac/service_editor_role.yaml">
# permissions for end users to edit services.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: service-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: service-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - services
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - services/status
  verbs:
  - get
</file>

<file path="config/rbac/service_viewer_role.yaml">
# permissions for end users to view services.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: service-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: service-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - services/status
  verbs:
  - get
</file>

<file path="config/rbac/sli_editor_role.yaml">
# permissions for end users to edit slis.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: sli-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: sli-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - slis
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - slis/status
  verbs:
  - get
</file>

<file path="config/rbac/sli_viewer_role.yaml">
# permissions for end users to view slis.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: sli-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: sli-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - slis
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - slis/status
  verbs:
  - get
</file>

<file path="config/rbac/slo_editor_role.yaml">
# permissions for end users to edit slos.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: slo-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: slo-editor-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - slos
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - slos/status
  verbs:
  - get
</file>

<file path="config/rbac/slo_viewer_role.yaml">
# permissions for end users to view slos.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: slo-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: slo-viewer-role
rules:
- apiGroups:
  - openslo.com
  resources:
  - slos
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - openslo.com
  resources:
  - slos/status
  verbs:
  - get
</file>

<file path="config/rbac/slo-kubernetes-operator_slo_editor_role.yaml">
# permissions for end users to edit slos.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: slo-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: slo-editor-role
rules:
- apiGroups:
  - slo-kubernetes-operator.openslo
  resources:
  - slos
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - slo-kubernetes-operator.openslo
  resources:
  - slos/status
  verbs:
  - get
</file>

<file path="config/rbac/slo-kubernetes-operator_slo_viewer_role.yaml">
# permissions for end users to view slos.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: slo-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: slo-kubernetes-operator
    app.kubernetes.io/part-of: slo-kubernetes-operator
    app.kubernetes.io/managed-by: kustomize
  name: slo-viewer-role
rules:
- apiGroups:
  - slo-kubernetes-operator.openslo
  resources:
  - slos
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - slo-kubernetes-operator.openslo
  resources:
  - slos/status
  verbs:
  - get
</file>

<file path="helm/osko/Chart.lock">
dependencies:
- name: prometheus-operator-crds
  repository: https://prometheus-community.github.io/helm-charts
  version: 8.0.1
- name: crds
  repository: ""
  version: 0.0.0
digest: sha256:ddfbe3e2e22cd500f81fe9910d0a2af266d98feeaaaed7ae1c050d5f00261220
generated: "2024-02-16T00:20:04.957964+01:00"
</file>

<file path="internal/controller/monitoring.coreos.com/suite_test.go">
package monitoringcoreoscom
⋮----
import (
	"fmt"
	"path/filepath"
	"runtime"
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	"k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/rest"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/envtest"
	logf "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"
	//+kubebuilder:scaffold:imports
)
⋮----
"fmt"
"path/filepath"
"runtime"
"testing"
⋮----
. "github.com/onsi/ginkgo/v2"
. "github.com/onsi/gomega"
⋮----
"k8s.io/client-go/kubernetes/scheme"
"k8s.io/client-go/rest"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/envtest"
logf "sigs.k8s.io/controller-runtime/pkg/log"
"sigs.k8s.io/controller-runtime/pkg/log/zap"
//+kubebuilder:scaffold:imports
⋮----
// These tests use Ginkgo (BDD-style Go testing framework). Refer to
// http://onsi.github.io/ginkgo/ to learn more about Ginkgo.
⋮----
var cfg *rest.Config
var k8sClient client.Client
var testEnv *envtest.Environment
⋮----
func TestControllers(t *testing.T)
⋮----
var _ = BeforeSuite(func() {
⋮----
// The BinaryAssetsDirectory is only required if you want to run the tests directly
// without call the makefile target test. If not informed it will look for the
// default path defined in controller-runtime which is /usr/local/kubebuilder/.
// Note that you must have the required binaries setup under the bin directory to perform
// the tests directly. When we run make test it will be setup and used automatically.
⋮----
var err error
// cfg is defined in this file globally.
⋮----
//+kubebuilder:scaffold:scheme
⋮----
var _ = AfterSuite(func() {
</file>

<file path="internal/controller/openslo/alertcondition_controller.go">
package controller
⋮----
import (
	"context"

	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
)
⋮----
"context"
⋮----
"k8s.io/apimachinery/pkg/runtime"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/log"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
⋮----
// AlertConditionReconciler reconciles a AlertCondition object
type AlertConditionReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}
⋮----
//+kubebuilder:rbac:groups=openslo.com,resources=alertconditions,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=openslo.com,resources=alertconditions/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=openslo.com,resources=alertconditions/finalizers,verbs=update
⋮----
func (r *AlertConditionReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *AlertConditionReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="internal/controller/openslo/alertnotificationtarget_controller.go">
package controller
⋮----
import (
	"context"

	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
)
⋮----
"context"
⋮----
"k8s.io/apimachinery/pkg/runtime"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/log"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
⋮----
// AlertNotificationTargetReconciler reconciles a AlertNotificationTarget object
type AlertNotificationTargetReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}
⋮----
//+kubebuilder:rbac:groups=openslo.com,resources=alertnotificationtargets,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=openslo.com,resources=alertnotificationtargets/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=openslo.com,resources=alertnotificationtargets/finalizers,verbs=update
⋮----
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.15.0/pkg/reconcile
func (r *AlertNotificationTargetReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *AlertNotificationTargetReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="internal/controller/openslo/alertpolicy_controller.go">
package controller
⋮----
import (
	"context"

	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
)
⋮----
"context"
⋮----
"k8s.io/apimachinery/pkg/runtime"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/log"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
⋮----
// AlertPolicyReconciler reconciles a AlertPolicy object
type AlertPolicyReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}
⋮----
//+kubebuilder:rbac:groups=openslo.com,resources=alertpolicies,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=openslo.com,resources=alertpolicies/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=openslo.com,resources=alertpolicies/finalizers,verbs=update
⋮----
func (r *AlertPolicyReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *AlertPolicyReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="internal/controller/openslo/suite_test.go">
package controller
⋮----
import (
	"path/filepath"
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	"k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/rest"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/envtest"
	logf "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	//+kubebuilder:scaffold:imports
)
⋮----
"path/filepath"
"testing"
⋮----
. "github.com/onsi/ginkgo/v2"
. "github.com/onsi/gomega"
⋮----
"k8s.io/client-go/kubernetes/scheme"
"k8s.io/client-go/rest"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/envtest"
logf "sigs.k8s.io/controller-runtime/pkg/log"
"sigs.k8s.io/controller-runtime/pkg/log/zap"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
//+kubebuilder:scaffold:imports
⋮----
// These tests use Ginkgo (BDD-style Go testing framework). Refer to
// http://onsi.github.io/ginkgo/ to learn more about Ginkgo.
⋮----
var cfg *rest.Config
var k8sClient client.Client
var testEnv *envtest.Environment
⋮----
func TestControllers(t *testing.T)
⋮----
var _ = BeforeSuite(func() {
⋮----
var err error
// cfg is defined in this file globally.
⋮----
//+kubebuilder:scaffold:scheme
⋮----
var _ = AfterSuite(func() {
</file>

<file path="internal/controller/osko/suite_test.go">
package osko
⋮----
import (
	"fmt"
	"path/filepath"
	"runtime"
	"testing"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"

	"k8s.io/client-go/kubernetes/scheme"
	"k8s.io/client-go/rest"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/envtest"
	logf "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/log/zap"

	oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
	//+kubebuilder:scaffold:imports
)
⋮----
"fmt"
"path/filepath"
"runtime"
"testing"
⋮----
. "github.com/onsi/ginkgo/v2"
. "github.com/onsi/gomega"
⋮----
"k8s.io/client-go/kubernetes/scheme"
"k8s.io/client-go/rest"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/envtest"
logf "sigs.k8s.io/controller-runtime/pkg/log"
"sigs.k8s.io/controller-runtime/pkg/log/zap"
⋮----
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
//+kubebuilder:scaffold:imports
⋮----
// These tests use Ginkgo (BDD-style Go testing framework). Refer to
// http://onsi.github.io/ginkgo/ to learn more about Ginkgo.
⋮----
var cfg *rest.Config
var k8sClient client.Client
var testEnv *envtest.Environment
⋮----
func TestControllers(t *testing.T)
⋮----
var _ = BeforeSuite(func() {
⋮----
// The BinaryAssetsDirectory is only required if you want to run the tests directly
// without call the makefile target test. If not informed it will look for the
// default path defined in controller-runtime which is /usr/local/kubebuilder/.
// Note that you must have the required binaries setup under the bin directory to perform
// the tests directly. When we run make test it will be setup and used automatically.
⋮----
var err error
// cfg is defined in this file globally.
⋮----
//+kubebuilder:scaffold:scheme
⋮----
var _ = AfterSuite(func() {
</file>

<file path="test/templates/tests/test-connection.yaml">
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "test.fullname" . }}-test-connection"
  labels:
    {{- include "test.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['{{ include "test.fullname" . }}:{{ .Values.service.port }}']
  restartPolicy: Never
</file>

<file path="test/templates/_helpers.tpl">
{{/*
Expand the name of the chart.
*/}}
{{- define "test.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "test.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "test.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "test.labels" -}}
helm.sh/chart: {{ include "test.chart" . }}
{{ include "test.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "test.selectorLabels" -}}
app.kubernetes.io/name: {{ include "test.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "test.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "test.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
</file>

<file path="test/templates/deployment.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "test.fullname" . }}
  labels:
    {{- include "test.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "test.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "test.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "test.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
</file>

<file path="test/templates/hpa.yaml">
{{- if .Values.autoscaling.enabled }}
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "test.fullname" . }}
  labels:
    {{- include "test.labels" . | nindent 4 }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "test.fullname" . }}
  minReplicas: {{ .Values.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.autoscaling.maxReplicas }}
  metrics:
    {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}
    {{- end }}
    {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}
    {{- end }}
{{- end }}
</file>

<file path="test/templates/ingress.yaml">
{{- if .Values.ingress.enabled -}}
{{- $fullName := include "test.fullname" . -}}
{{- $svcPort := .Values.service.port -}}
{{- if and .Values.ingress.className (not (semverCompare ">=1.18-0" .Capabilities.KubeVersion.GitVersion)) }}
  {{- if not (hasKey .Values.ingress.annotations "kubernetes.io/ingress.class") }}
  {{- $_ := set .Values.ingress.annotations "kubernetes.io/ingress.class" .Values.ingress.className}}
  {{- end }}
{{- end }}
{{- if semverCompare ">=1.19-0" .Capabilities.KubeVersion.GitVersion -}}
apiVersion: networking.k8s.io/v1
{{- else if semverCompare ">=1.14-0" .Capabilities.KubeVersion.GitVersion -}}
apiVersion: networking.k8s.io/v1beta1
{{- else -}}
apiVersion: extensions/v1beta1
{{- end }}
kind: Ingress
metadata:
  name: {{ $fullName }}
  labels:
    {{- include "test.labels" . | nindent 4 }}
  {{- with .Values.ingress.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  {{- if and .Values.ingress.className (semverCompare ">=1.18-0" .Capabilities.KubeVersion.GitVersion) }}
  ingressClassName: {{ .Values.ingress.className }}
  {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
    {{- range .Values.ingress.tls }}
    - hosts:
        {{- range .hosts }}
        - {{ . | quote }}
        {{- end }}
      secretName: {{ .secretName }}
    {{- end }}
  {{- end }}
  rules:
    {{- range .Values.ingress.hosts }}
    - host: {{ .host | quote }}
      http:
        paths:
          {{- range .paths }}
          - path: {{ .path }}
            {{- if and .pathType (semverCompare ">=1.18-0" $.Capabilities.KubeVersion.GitVersion) }}
            pathType: {{ .pathType }}
            {{- end }}
            backend:
              {{- if semverCompare ">=1.19-0" $.Capabilities.KubeVersion.GitVersion }}
              service:
                name: {{ $fullName }}
                port:
                  number: {{ $svcPort }}
              {{- else }}
              serviceName: {{ $fullName }}
              servicePort: {{ $svcPort }}
              {{- end }}
          {{- end }}
    {{- end }}
{{- end }}
</file>

<file path="test/templates/NOTES.txt">
1. Get the application URL by running these commands:
{{- if .Values.ingress.enabled }}
{{- range $host := .Values.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
  {{- end }}
{{- end }}
{{- else if contains "NodePort" .Values.service.type }}
  export NODE_PORT=$(kubectl get --namespace {{ .Release.Namespace }} -o jsonpath="{.spec.ports[0].nodePort}" services {{ include "test.fullname" . }})
  export NODE_IP=$(kubectl get nodes --namespace {{ .Release.Namespace }} -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT
{{- else if contains "LoadBalancer" .Values.service.type }}
     NOTE: It may take a few minutes for the LoadBalancer IP to be available.
           You can watch the status of by running 'kubectl get --namespace {{ .Release.Namespace }} svc -w {{ include "test.fullname" . }}'
  export SERVICE_IP=$(kubectl get svc --namespace {{ .Release.Namespace }} {{ include "test.fullname" . }} --template "{{"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}"}}")
  echo http://$SERVICE_IP:{{ .Values.service.port }}
{{- else if contains "ClusterIP" .Values.service.type }}
  export POD_NAME=$(kubectl get pods --namespace {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "test.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  export CONTAINER_PORT=$(kubectl get pod --namespace {{ .Release.Namespace }} $POD_NAME -o jsonpath="{.spec.containers[0].ports[0].containerPort}")
  echo "Visit http://127.0.0.1:8080 to use your application"
  kubectl --namespace {{ .Release.Namespace }} port-forward $POD_NAME 8080:$CONTAINER_PORT
{{- end }}
</file>

<file path="test/templates/service.yaml">
apiVersion: v1
kind: Service
metadata:
  name: {{ include "test.fullname" . }}
  labels:
    {{- include "test.labels" . | nindent 4 }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: http
      protocol: TCP
      name: http
  selector:
    {{- include "test.selectorLabels" . | nindent 4 }}
</file>

<file path="test/templates/serviceaccount.yaml">
{{- if .Values.serviceAccount.create -}}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "test.serviceAccountName" . }}
  labels:
    {{- include "test.labels" . | nindent 4 }}
  {{- with .Values.serviceAccount.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
{{- end }}
</file>

<file path="test/.helmignore">
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*.orig
*~
# Various IDEs
.project
.idea/
*.tmproj
.vscode/
</file>

<file path="test/Chart.yaml">
apiVersion: v2
name: test
description: A Helm chart for Kubernetes

# A chart can be either an 'application' or a 'library' chart.
#
# Application charts are a collection of templates that can be packaged into versioned archives
# to be deployed.
#
# Library charts provide useful utilities or functions for the chart developer. They're included as
# a dependency of application charts to inject those utilities and functions into the rendering
# pipeline. Library charts do not define any templates and therefore cannot be deployed.
type: application

# This is the chart version. This version number should be incremented each time you make changes
# to the chart and its templates, including the app version.
# Versions are expected to follow Semantic Versioning (https://semver.org/)
version: 0.1.0

# This is the version number of the application being deployed. This version number should be
# incremented each time you make changes to the application. Versions are not expected to
# follow Semantic Versioning. They should reflect the version the application is using.
# It is recommended to use it with quotes.
appVersion: "1.16.0"
</file>

<file path="test/values.yaml">
# Default values for test.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: nginx
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
</file>

<file path=".dockerignore">
# More info: https://docs.docker.com/engine/reference/builder/#dockerignore-file
# Ignore build and test binaries.
bin/
</file>

<file path=".gitignore">
# Binaries for programs and plugins
*.exe
*.exe~
*.dll
*.so
*.dylib
bin/*
Dockerfile.cross

# Test binary, build with `go test -c`
*.test

# Output of the go coverage tool, specifically when used with LiteIDE
*.out

# Kubernetes Generated files - skip generated files, except for vendored files

!vendor/**/zz_generated.*

# editor and IDE paraphernalia
.idea
*.swp
*.swo
*~
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
config/crd/**/*
assets/
</file>

<file path="DCO">
Developer Certificate of Origin
Version 1.1

Everyone is permitted to copy and distribute verbatim copies of this
license document, but changing it is not allowed.

Developer's Certificate of Origin 1.1

By making a contribution to this project, I certify that:

(a) The contribution was created in whole or in part by me and I
    have the right to submit it under the open source license
    indicated in the file; or

(b) The contribution is based upon previous work that, to the best
    of my knowledge, is covered under an appropriate open source
    license and I have the right under that license to submit that
    work with modifications, whether created in whole or in part
    by me, under the same open source license (unless I am
    permitted to submit under a different license), as indicated
    in the file; or

(c) The contribution was provided directly to me by some other
    person who certified (a), (b) or (c) and I have not modified
    it.

(d) I understand and agree that this project and the contribution
    are public and that a record of the contribution (including all
    personal information I submit with it, including my sign-off) is
    maintained indefinitely and may be redistributed consistent with
    this project or the open source license(s) involved.
</file>

<file path="entitlements.xml">
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd"\>
<plist version="1.0">
<dict>
    <key>com.apple.security.hypervisor</key>
    <true/>
</dict>
</plist>
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.
</file>

<file path="repomix.config.json">
{
  "$schema": "https://repomix.com/schemas/latest/schema.json",
  "input": {
    "maxFileSize": 52428800
  },
  "output": {
    "filePath": "repomix-output.xml",
    "style": "xml",
    "parsableStyle": false,
    "fileSummary": true,
    "directoryStructure": true,
    "files": true,
    "removeComments": false,
    "removeEmptyLines": false,
    "compress": false,
    "topFilesLength": 5,
    "showLineNumbers": false,
    "copyToClipboard": false,
    "git": {
      "sortByChanges": true,
      "sortByChangesMaxCommits": 100,
      "includeDiffs": false
    }
  },
  "include": [],
  "ignore": {
    "useGitignore": true,
    "useDefaultPatterns": true,
    "customPatterns": []
  },
  "security": {
    "enableSecurityCheck": true
  },
  "tokenCount": {
    "encoding": "o200k_base"
  }
}
</file>

<file path=".github/workflows/publish-image.yaml">
name: Create and publish the Docker image

on:
  push:
    branches:
      - main
    tags:
        - '0.*.*'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push-image:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Log in to the Container registry
        uses: docker/login-action@65b78e6e13532edd9afa3aa52ac7964289d1a9c1
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@9ec57ed1fcdbf14dcef7dfbe97b2010124a938b7
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=tag
            type=raw,value=latest,enable={{is_default_branch}}
            type=ref,event=branch,suffix=-{{sha}}

      - name: Build and push Docker image
        uses: docker/build-push-action@f2a1d5e99d037542a71f64918e516c093c6f3fc4
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
</file>

<file path=".github/workflows/release-drafter.yml">
name: Release Drafter

on:
  push:
    # branches to consider in the event; optional, defaults to all
    branches:
      - main
  # pull_request event is required only for autolabeler
  pull_request:
    # Only following types are handled by the action, but one can default to all as well
    types: [opened, reopened, synchronize]
  # pull_request_target event is required for autolabeler to support PRs from forks
  # pull_request_target:
  #   types: [opened, reopened, synchronize]

permissions:
  contents: read

jobs:
  update_release_draft:
    permissions:
      # write permission is required to create a github release
      contents: write
      # write permission is required for autolabeler
      # otherwise, read permission is required at least
      pull-requests: write
    runs-on: ubuntu-latest
    steps:
      # (Optional) GitHub Enterprise requires GHE_HOST variable set
      #- name: Set GHE_HOST
      #  run: |
      #    echo "GHE_HOST=${GITHUB_SERVER_URL##https:\/\/}" >> $GITHUB_ENV

      # Drafts your next Release notes as Pull Requests are merged into "master"
      - uses: release-drafter/release-drafter@v6
        # (Optional) specify config name to use, relative to .github/. Default: release-drafter.yml
        # with:
        #   config-name: my-config.yml
        #   disable-autolabeler: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path=".github/release-drafter.yml">
category-template: '## $TITLE'
name-template: 'v$RESOLVED_VERSION'
tag-template: 'v$RESOLVED_VERSION'
tag-prefix: ''
version-template: $MAJOR.$MINOR.$PATCH
change-template: '* $TITLE (#$NUMBER) by @$AUTHOR'
change-title-escapes: ''
no-changes-template: 'No changes were made in this version. Stay tuned for upcoming updates!'
categories:
  - title: '⚡ Breaking Changes'
    labels:
      - 'breaking-change'
  - title: '🌟 New Features'
    labels:
      - 'feature'
  - title: '🔧 Improvements'
    labels:
      - 'enhancement'
  - title: '📜 Documentation Updates'
    labels:
      - 'documentation'
  - title: '🐛 Bug Fixes'
    labels:
      - 'bug'
  - title: '🚒 Deprecations'
    labels:
      - 'deprecation'
  - title: '🔧 Maintenance'
    labels:
      - 'chore'
  - title: '📦 Dependency Updates'
    collapse-after: 10
    labels:
      - 'dependencies'
version-resolver:
  major:
    labels:
      - 'major'
      - 'breaking-change'
  minor:
    labels:
      - 'minor'
      - 'feature'
      - 'enhancement'
      - 'deprecation'
  patch:
    labels:
      - 'patch'
      - 'documentation'
      - 'bug'
      - 'bugfix'
      - 'fix'
      - 'chore'
      - 'internal'
      - 'dependencies'
  default: patch
autolabeler:
  - label: 'breaking-change'
    title:
      - '/.*!:.*/'
  - label: 'feature'
    title:
      - '/feat.*: /i'
  - label: 'bug'
    title:
      - '/fix.*: /i'
      - '/bug.*: /i'
  - label: 'dependencies'
    branch:
      - '/dependabot\/.*/'
  - label: 'documentation'
    files:
      - '*.md'
  - label: 'chore'
    files:
      - '*.md'
exclude-labels:
  - 'skip-changelog'
template: |
  ## Summary

  **[Human readable summary of changes]**

  ## Changes

  $CHANGES

  ## This release was made possible by the following contributors:

  $CONTRIBUTORS
</file>

<file path="api/openslo/v1/common_types.go">
package v1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
type ObjectMetaOpenSLO struct {
	metav1.ObjectMeta `json:",inline"`
	DisplayName       string `json:"displayName,omitempty"`
}
⋮----
// +kubebuilder:validation:MaxLength=1050
type Description string
⋮----
// +kubebuilder:validation:Pattern=`^[1-9]\d*[s m h d]$`
type Duration string
⋮----
type MetricSource struct {
	MetricSourceRef string           `json:"metricSourceRef,omitempty"`
	Type            string           `json:"type,omitempty"`
	Spec            MetricSourceSpec `json:"spec,omitempty"`
}
⋮----
type MetricSourceSpec struct {
	Query string `json:"query,omitempty"`
}
</file>

<file path="api/openslo/v1/sli_types.go">
package v1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
type MetricSpec struct {
	MetricSource MetricSource `json:"metricSource,omitempty"`
}
⋮----
type RatioMetricSpec struct {
	Raw MetricSpec `json:"raw,omitempty"`
	// +kubebuilder:validation:Enum=success;failure
	RawType string     `json:"rawType,omitempty"`
	Good    MetricSpec `json:"good,omitempty"`
	Bad     MetricSpec `json:"bad,omitempty"`
	Total   MetricSpec `json:"total,omitempty"`
	Counter bool       `json:"counter,omitempty"`
}
⋮----
// +kubebuilder:validation:Enum=success;failure
⋮----
type ThresholdMetricSpec struct {
	MetricSource MetricSource `json:"metricSource,omitempty"`
}
⋮----
// SLISpec defines the desired state of SLI
type SLISpec struct {
	Description     Description         `json:"description,omitempty"`
	ThresholdMetric ThresholdMetricSpec `json:"thresholdMetric,omitempty"`
	RatioMetric     RatioMetricSpec     `json:"ratioMetric,omitempty"`
}
⋮----
// SLIStatus defines the observed state of SLI
type SLIStatus struct {
	// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
	// Important: Run "make" to regenerate code after modifying this file
}
⋮----
// INSERT ADDITIONAL STATUS FIELD - define observed state of cluster
// Important: Run "make" to regenerate code after modifying this file
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
⋮----
// SLI is the Schema for the slis API
type SLI struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   SLISpec   `json:"spec,omitempty"`
	Status SLIStatus `json:"status,omitempty"`
}
⋮----
// SLIList contains a list of SLI
type SLIList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []SLI `json:"items"`
}
⋮----
func init()
</file>

<file path="config/rbac/kustomization.yaml">
resources:
# All RBAC will be applied under this service account in
# the deployment namespace. You may comment out this resource
# if your manager will use a service account that exists at
# runtime. Be sure to update RoleBinding and ClusterRoleBinding
# subjects if changing service account names.
- service_account.yaml
- role.yaml
- role_binding.yaml
- leader_election_role.yaml
- leader_election_role_binding.yaml
# Comment the following 4 lines if you want to disable
# the auth proxy (https://github.com/brancz/kube-rbac-proxy)
# which protects your /metrics endpoint.
- auth_proxy_service.yaml
- auth_proxy_role.yaml
- auth_proxy_role_binding.yaml
- auth_proxy_client_clusterrole.yaml
# For each CRD, "Editor" and "Viewer" roles are scaffolded by
# default, aiding admins in cluster management. Those roles are
# not used by the Project itself. You can comment the following lines
# if you do not want those helpers be installed with your Project.
- osko_alertmanagerconfig_editor_role.yaml
- osko_alertmanagerconfig_viewer_role.yaml
</file>

<file path="config/rbac/osko_alertmanagerconfig_editor_role.yaml">
# permissions for end users to edit alertmanagerconfigs.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: osko
    app.kubernetes.io/managed-by: kustomize
  name: osko-alertmanagerconfig-editor-role
rules:
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs/status
  verbs:
  - get
</file>

<file path="config/rbac/osko_alertmanagerconfig_viewer_role.yaml">
# permissions for end users to view alertmanagerconfigs.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: osko
    app.kubernetes.io/managed-by: kustomize
  name: osko-alertmanagerconfig-viewer-role
rules:
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs/status
  verbs:
  - get
</file>

<file path="config/rbac/osko_mimirrule_editor_role.yaml">
# permissions for end users to edit mimirrules.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: mimirrule-editor-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: osko
    app.kubernetes.io/part-of: osko
    app.kubernetes.io/managed-by: kustomize
  name: mimirrule-editor-role
rules:
- apiGroups:
  - osko.dev
  resources:
  - mimirrules
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - osko.dev
  resources:
  - mimirrules/status
  verbs:
  - get
</file>

<file path="config/rbac/osko_mimirrule_viewer_role.yaml">
# permissions for end users to view mimirrules.
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/name: clusterrole
    app.kubernetes.io/instance: mimirrule-viewer-role
    app.kubernetes.io/component: rbac
    app.kubernetes.io/created-by: osko
    app.kubernetes.io/part-of: osko
    app.kubernetes.io/managed-by: kustomize
  name: mimirrule-viewer-role
rules:
- apiGroups:
  - osko.dev
  resources:
  - mimirrules
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - osko.dev
  resources:
  - mimirrules/status
  verbs:
  - get
</file>

<file path="config/samples/slo_gatekeeper_webhook_response_time_0005.yaml">
apiVersion: openslo.com/v1
kind: SLO
metadata:
  name: gatekeeper-webhook-response-time-0005s
  namespace: default
  annotations:
    osko.dev/datasourceRef: "mimir-infra-ds"
spec:
  budgetingMethod: Occurrences
  description: 99% of Gatekeeper webhook requests return in less than 0.0005s
  indicator:
    metadata:
      name: gatekeeper-webhook-response-time-0005s
    spec:
      description: 99% of Gatekeeper webhook requests return in less than 0.0005s
      ratioMetric:
        good:
          metricSource:
            metricSourceRef: mimir
            spec:
              query: controller_runtime_webhook_latency_seconds_bucket{le="0.0005"}
        total:
          metricSource:
            metricSourceRef: mimir
            spec:
              query: controller_runtime_webhook_latency_seconds_count{}
  objectives:
    - displayName: gatekeeper-webhook-response-time-0005s
      target: '0.99'
  service: gatekeeper
  timeWindow:
    - duration: 28d
      isRolling: true
</file>

<file path="devel/grafana/deployment.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  labels:
    app: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:latest
        ports:
        - containerPort: 3000
        volumeMounts:
        - name: grafana-datasources
          mountPath: /etc/grafana/provisioning/datasources
          readOnly: true
        env:
        - name: GF_SERVER_ROOT_URL
          value: http://localhost:3000
        - name: GF_SECURITY_ADMIN_USER
          value: admin
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: admin
      securityContext:
        runAsUser: 472
        fsGroup: 472
      volumes:
      - name: grafana-datasources
        configMap:
          name: grafana-datasources
</file>

<file path="devel/grafana/mimir-datasource.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  labels:
    app: grafana
data:
  datasources.yaml: |-
    apiVersion: 1
    datasources:
    - name: Mimir
      type: prometheus
      access: proxy
      url: http://mimir-service:9009/prometheus
      isDefault: true
      jsonData:
        httpHeaderName1: X-Scope-OrgID
      secureJsonData:
        httpHeaderValue1: monitoring
</file>

<file path="devel/grafana/osko-1.json">
{
  "description": "Overview of systems",
  "graphTooltip": 1,
  "panels": [
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "panels": [],
      "title": "Status of systems owned by $team",
      "type": "row"
    },
    {
      "datasource": {
        "type": "datasource",
        "uid": "-- Mixed --"
      },
      "fieldConfig": {
        "defaults": {
          "thresholds": {
            "steps": [
              {
                "color": "red",
                "value": null
              },
              {
                "color": "green",
                "value": 1
              }
            ]
          },
          "unit": "bool_yes_no"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 1
      },
      "id": 2,
      "links": [
        {
          "url": "http://localhost:3000/d/osko-slos/osko-slos?orgId=1&${system:queryparam}&${team:queryparam}&${window:queryparam}&${env:queryparam}&${domain:queryparam}"
        }
      ],
      "options": {
        "colorMode": "value",
        "graphMode": "area",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ]
        }
      },
      "pluginVersion": "v10.4.0",
      "repeat": "system",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "$env"
          },
          "expr": "min(\n    sum(\n        osko_sli_measurement{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", window=~\"$window\"}\n    ) by (domain, system)\n    > bool\n    sum(\n        osko_slo_target{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", window=~\"$window\"}\n    ) by (domain, system)\n) by (system)\n"
        }
      ],
      "title": "$system",
      "type": "stat"
    },
    {
      "collapsed": false,
      "gridPos": {
        "h": 1,
        "w": 24,
        "x": 0,
        "y": 9
      },
      "id": 3,
      "panels": [],
      "repeat": "system",
      "title": "Lowest SLI of $system",
      "type": "row"
    },
    {
      "datasource": {
        "type": "datasource",
        "uid": "-- Mixed --"
      },
      "fieldConfig": {
        "defaults": {
          "decimals": 4,
          "max": 1,
          "min": 0,
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "red",
                "value": null
              },
              {
                "color": "green",
                "value": 0.98999999999999999
              }
            ]
          },
          "unit": "percentunit"
        }
      },
      "gridPos": {
        "h": 8,
        "w": 24,
        "x": 0,
        "y": 10
      },
      "id": 4,
      "options": {
        "displayMode": "basic",
        "minVizHeight": 10,
        "minVizWidth": 0,
        "orientation": "horizontal",
        "reduceOptions": {
          "calcs": [
            "lastNotNull"
          ]
        },
        "showUnfilled": true,
        "valueMode": "color"
      },
      "pluginVersion": "v10.4.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "$env"
          },
          "expr": "min(\n    osko_sli_measurement{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", window=~\"$window\"}\n) by (system)\n"
        }
      ],
      "title": "$system",
      "type": "bargauge"
    }
  ],
  "schemaVersion": 36,
  "templating": {
    "list": [
      {
        "description": "Environment",
        "label": "Env",
        "name": "env",
        "query": "prometheus",
        "regex": "mimir-.*",
        "type": "datasource"
      },
      {
        "current": {
          "selected": false,
          "text": "28d",
          "value": "28d"
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${env}"
        },
        "description": "Window",
        "label": "Window",
        "name": "window",
        "query": "label_values(osko_slo_target, window)",
        "refresh": 2,
        "type": "query"
      },
      {
        "current": {
          "selected": false,
          "text": [
            "All"
          ],
          "value": [
            "$__all"
          ]
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${env}"
        },
        "description": "Team",
        "includeAll": true,
        "label": "Team",
        "multi": true,
        "name": "team",
        "query": "label_values(osko_slo_target, team)",
        "refresh": 2,
        "type": "query"
      },
      {
        "current": {
          "selected": false,
          "text": [
            "All"
          ],
          "value": [
            "$__all"
          ]
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${env}"
        },
        "description": "Domain",
        "includeAll": true,
        "label": "Domain",
        "multi": true,
        "name": "domain",
        "query": "label_values(osko_slo_target{team=~\"$team\"}, domain)",
        "refresh": 2,
        "type": "query"
      },
      {
        "current": {
          "selected": false,
          "text": [
            "All"
          ],
          "value": [
            "$__all"
          ]
        },
        "datasource": {
          "type": "prometheus",
          "uid": "${env}"
        },
        "description": "System",
        "includeAll": true,
        "label": "System",
        "multi": true,
        "name": "system",
        "query": "label_values(osko_slo_target{team=~\"$team\", domain=~\"$domain\"}, system)",
        "refresh": 2,
        "type": "query"
      }
    ]
  },
  "time": {
    "from": "now-6h",
    "to": "now"
  },
  "timezone": "utc",
  "title": "OSKO / Systems overview",
  "uid": "osko-systems-overview"
}
</file>

<file path="devel/grafana/osko-2.json">
{
   "description": "Look at specific SLOs",
   "graphTooltip": 1,
   "panels": [
      {
         "collapsed": false,
         "gridPos": {
            "h": 1,
            "w": 24,
            "x": 0,
            "y": 0
         },
         "id": 1,
         "panels": [ ],
         "repeat": "slo",
         "title": "$slo SLO",
         "type": "row"
      },
      {
         "datasource": {
            "type": "datasource",
            "uid": "-- Mixed --"
         },
         "fieldConfig": {
            "defaults": {
               "thresholds": {
                  "steps": [
                     {
                        "color": "red",
                        "value": null
                     },
                     {
                        "color": "green",
                        "value": 1
                     }
                  ]
               },
               "unit": "bool_yes_no"
            }
         },
         "gridPos": {
            "h": 8,
            "w": 4,
            "x": 0,
            "y": 1
         },
         "id": 2,
         "options": {
            "colorMode": "value",
            "graphMode": "area",
            "reduceOptions": {
               "calcs": [
                  "lastNotNull"
               ]
            }
         },
         "pluginVersion": "v10.4.0",
         "targets": [
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "sum(\n    osko_sli_measurement{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n) by (slo_name)\n> bool\nsum(\n    osko_slo_target{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n) by (slo_name)\n"
            }
         ],
         "title": "Passing",
         "type": "stat"
      },
      {
         "datasource": {
            "type": "datasource",
            "uid": "-- Mixed --"
         },
         "fieldConfig": {
            "defaults": {
               "custom": {
                  "showPoints": "never"
               },
               "decimals": 3,
               "max": 1,
               "unit": "percentunit"
            }
         },
         "gridPos": {
            "h": 8,
            "w": 4,
            "x": 4,
            "y": 1
         },
         "id": 3,
         "pluginVersion": "v10.4.0",
         "targets": [
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "min(\n    osko_sli_measurement{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n) by (slo_name)\n",
               "legendFormat": "Measurement"
            },
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "min(\n    osko_slo_target{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n) by (slo_name)\n",
               "legendFormat": "Target"
            }
         ],
         "title": "SLI vs SLO",
         "type": "timeseries"
      },
      {
         "datasource": {
            "type": "datasource",
            "uid": "-- Mixed --"
         },
         "fieldConfig": {
            "defaults": {
               "custom": {
                  "showPoints": "never"
               },
               "unit": "short"
            }
         },
         "gridPos": {
            "h": 8,
            "w": 4,
            "x": 8,
            "y": 1
         },
         "id": 4,
         "pluginVersion": "v10.4.0",
         "targets": [
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "min(\n    osko_error_budget_burn_rate{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n) by (slo_name)\n",
               "legendFormat": "Burn rate {{ window }}"
            },
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "vector(1)\n",
               "legendFormat": "Threshold {{ window }}"
            }
         ],
         "title": "Error Budget burn rate",
         "type": "timeseries"
      },
      {
         "datasource": {
            "type": "datasource",
            "uid": "-- Mixed --"
         },
         "fieldConfig": {
            "defaults": {
               "color": {
                  "fixedColor": "purple",
                  "mode": "fixed"
               },
               "thresholds": {
                  "mode": "absolute"
               },
               "unit": "short"
            }
         },
         "gridPos": {
            "h": 8,
            "w": 4,
            "x": 12,
            "y": 1
         },
         "id": 5,
         "options": {
            "colorMode": "value",
            "graphMode": "area",
            "reduceOptions": {
               "calcs": [
                  "lastNotNull"
               ]
            }
         },
         "pluginVersion": "v10.4.0",
         "targets": [
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "sum(osko_sli_total{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"})\n"
            }
         ],
         "title": "Total events",
         "type": "stat"
      },
      {
         "datasource": {
            "type": "datasource",
            "uid": "-- Mixed --"
         },
         "fieldConfig": {
            "defaults": {
               "thresholds": {
                  "mode": "absolute",
                  "steps": [
                     {
                        "color": "red",
                        "value": null
                     },
                     {
                        "color": "green",
                        "value": 1
                     }
                  ]
               },
               "unit": "short"
            }
         },
         "gridPos": {
            "h": 8,
            "w": 4,
            "x": 16,
            "y": 1
         },
         "id": 6,
         "options": {
            "colorMode": "value",
            "graphMode": "area",
            "reduceOptions": {
               "calcs": [
                  "lastNotNull"
               ]
            }
         },
         "pluginVersion": "v10.4.0",
         "targets": [
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "sum(\n    osko_error_budget_target{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n    *\n    osko_sli_total{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n)\n",
               "legendFormat": "Budget"
            }
         ],
         "title": "Errors budget",
         "type": "stat"
      },
      {
         "datasource": {
            "type": "datasource",
            "uid": "-- Mixed --"
         },
         "fieldConfig": {
            "defaults": {
               "thresholds": {
                  "mode": "absolute",
                  "steps": [
                     {
                        "color": "red",
                        "value": null
                     },
                     {
                        "color": "green",
                        "value": 1
                     }
                  ]
               },
               "unit": "short"
            }
         },
         "gridPos": {
            "h": 8,
            "w": 4,
            "x": 20,
            "y": 1
         },
         "id": 7,
         "options": {
            "colorMode": "value",
            "graphMode": "area",
            "reduceOptions": {
               "calcs": [
                  "lastNotNull"
               ]
            }
         },
         "pluginVersion": "v10.4.0",
         "targets": [
            {
               "datasource": {
                  "type": "prometheus",
                  "uid": "$env"
               },
               "expr": "sum(\n    (\n        osko_error_budget_target{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n        - osko_error_budget_available{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n    ) *\n    osko_sli_total{domain=~\"$domain\", system=~\"$system\", team=~\"$team\", service=~\"$service\", slo_name=~\"$slo\", window=\"$window\"}\n)\n",
               "legendFormat": "Remaining"
            }
         ],
         "title": "Errors remaining",
         "type": "stat"
      }
   ],
   "schemaVersion": 36,
   "tags": [
      "osko"
   ],
   "templating": {
      "list": [
         {
            "description": "Environment",
            "label": "Env",
            "name": "env",
            "query": "prometheus",
            "regex": "mimir-.*",
            "type": "datasource"
         },
         {
            "current": {
               "selected": false,
               "text": "28d",
               "value": "28d"
            },
            "datasource": {
               "type": "prometheus",
               "uid": "${env}"
            },
            "description": "Window",
            "label": "Window",
            "name": "window",
            "query": "label_values(osko_slo_target, window)",
            "refresh": 2,
            "type": "query"
         },
         {
            "current": {
               "selected": false,
               "text": [
                  "All"
               ],
               "value": [
                  "$__all"
               ]
            },
            "datasource": {
               "type": "prometheus",
               "uid": "${env}"
            },
            "description": "Team",
            "includeAll": true,
            "label": "Team",
            "multi": true,
            "name": "team",
            "query": "label_values(osko_slo_target, team)",
            "refresh": 2,
            "type": "query"
         },
         {
            "current": {
               "selected": false,
               "text": [
                  "All"
               ],
               "value": [
                  "$__all"
               ]
            },
            "datasource": {
               "type": "prometheus",
               "uid": "${env}"
            },
            "description": "Domain",
            "includeAll": true,
            "label": "Domain",
            "multi": true,
            "name": "domain",
            "query": "label_values(osko_slo_target{team=~\"$team\"}, domain)",
            "refresh": 2,
            "type": "query"
         },
         {
            "current": {
               "selected": false,
               "text": [
                  "All"
               ],
               "value": [
                  "$__all"
               ]
            },
            "datasource": {
               "type": "prometheus",
               "uid": "${env}"
            },
            "description": "System",
            "includeAll": true,
            "label": "System",
            "multi": true,
            "name": "system",
            "query": "label_values(osko_slo_target{team=~\"$team\", domain=~\"$domain\"}, system)",
            "refresh": 2,
            "type": "query"
         },
         {
            "current": {
               "selected": false,
               "text": [
                  "All"
               ],
               "value": [
                  "$__all"
               ]
            },
            "datasource": {
               "type": "prometheus",
               "uid": "${env}"
            },
            "description": "Service",
            "includeAll": true,
            "label": "Service",
            "multi": true,
            "name": "service",
            "query": "label_values(osko_slo_target{team=~\"$team\", domain=~\"$domain\", system=~\"$system\"}, service)",
            "refresh": 2,
            "type": "query"
         },
         {
            "current": {
               "selected": false,
               "text": [
                  "All"
               ],
               "value": [
                  "$__all"
               ]
            },
            "datasource": {
               "type": "prometheus",
               "uid": "${env}"
            },
            "description": "SLO",
            "includeAll": true,
            "label": "SLO",
            "multi": true,
            "name": "slo",
            "query": "label_values(osko_slo_target{team=~\"$team\", domain=~\"$domain\", system=~\"$system\", service=~\"$service\"}, slo_name)",
            "refresh": 2,
            "type": "query"
         }
      ]
   },
   "time": {
      "from": "now-6h",
      "to": "now"
   },
   "timezone": "utc",
   "title": "OSKO / SLOs",
   "uid": "osko-slos"
}
</file>

<file path="devel/grafana/service.yaml">
apiVersion: v1
kind: Service
metadata:
  name: grafana
  labels:
    app: grafana
spec:
  type: ClusterIP
  ports:
  - port: 3000
    targetPort: 3000
    protocol: TCP
    name: grafana
  selector:
    app: grafana
</file>

<file path="devel/metrics-generator/metrics-generator.go">
package main
⋮----
import (
	"fmt"
	"math/rand"
	"net/http"
)
⋮----
"fmt"
"math/rand"
"net/http"
⋮----
// MetricData holds the simulated metrics data
type MetricData struct {
	TotalRequests int
	ErrorCount    int
}
⋮----
// GenerateMetrics simulates the generation of metrics data.
func GenerateMetrics(data *MetricData)
⋮----
// Simulate total requests
⋮----
data.TotalRequests += currentTotalRequests // Increment total requests by a random number up to 100
⋮----
// Simulate error count based on a 4.5% error rate of new requests
⋮----
// metricsHandler outputs metrics in a format that Prometheus can scrape.
func metricsHandler(data *MetricData) http.HandlerFunc
⋮----
// Generate new metrics data
⋮----
// Output metrics in Prometheus exposition format
⋮----
func sum(slice []float64) (total float64)
⋮----
func main()
⋮----
// Initialize MetricData
</file>

<file path="devel/mimir/service.yaml">
apiVersion: v1
kind: Service
metadata:
  name: mimir-service
spec:
  selector:
    app: mimir
  ports:
  - protocol: TCP
    port: 9009
    targetPort: 9009
  type: ClusterIP
</file>

<file path="devel/kustomization.yaml">
resources:
  - grafana/deployment.yaml
  - grafana/service.yaml
  - grafana/mimir-datasource.yaml
  - mimir/deployment.yaml
  - mimir/service.yaml
  - mimir/configmap.yaml
  - mimir/alertmanager-default-config.yaml
  - grafana-agent/deployment.yaml
  - grafana-agent/configmap.yaml
</file>

<file path="helm/osko/charts/crds/Chart.yaml">
apiVersion: v2
name: crds
version: 0.0.0
</file>

<file path="helm/osko/templates/tests/test-connection.yaml">
apiVersion: v1
kind: Pod
metadata:
  name: "{{ include "osko.fullname" . }}-test-connection"
  labels:
    {{- include "osko.labels" . | nindent 4 }}
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['{{ include "osko.fullname" . }}:8081']
  restartPolicy: Never
</file>

<file path="helm/osko/templates/_helpers.tpl">
{{/*
Expand the name of the chart.
*/}}
{{- define "osko.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
We truncate at 63 chars because some Kubernetes name fields are limited to this (by the DNS naming spec).
If release name contains chart name it will be used as a full name.
*/}}
{{- define "osko.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "osko.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "osko.labels" -}}
helm.sh/chart: {{ include "osko.chart" . }}
{{ include "osko.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "osko.selectorLabels" -}}
app.kubernetes.io/name: {{ include "osko.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "osko.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "osko.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
</file>

<file path="helm/osko/templates/deployment.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "osko.fullname" . }}
  labels:
    {{- include "osko.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "osko.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "osko.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "osko.serviceAccountName" . }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          command:
            - /manager
          args:
            - --leader-elect
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: http
          readinessProbe:
            httpGet:
              path: /readyz
              port: http
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      terminationGracePeriodSeconds: 10
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
</file>

<file path="helm/osko/templates/hpa.yaml">
{{- if .Values.autoscaling.enabled }}
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: {{ include "osko.fullname" . }}
  labels:
    {{- include "osko.labels" . | nindent 4 }}
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: {{ include "osko.fullname" . }}
  minReplicas: {{ .Values.autoscaling.minReplicas }}
  maxReplicas: {{ .Values.autoscaling.maxReplicas }}
  metrics:
    {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}
    - type: Resource
      resource:
        name: cpu
        targetAverageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}
    {{- end }}
    {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}
    - type: Resource
      resource:
        name: memory
        targetAverageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}
    {{- end }}
{{- end }}
</file>

<file path="helm/osko/templates/leader_election_role_binding.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    {{ include "osko.labels" . | nindent 4 }}
  name: leader-election-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: {{ include "osko.fullname" . }}-leader-election-role
subjects:
- kind: ServiceAccount
  name: {{ include "osko.fullname" . }}
  namespace: {{ .Release.Namespace }}
</file>

<file path="helm/osko/templates/leader_election_role.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    {{ include "osko.labels" . | nindent 4 }}
  name: {{ include "osko.fullname" . }}-leader-election-role
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
</file>

<file path="helm/osko/templates/NOTES.txt">
1. OpenSLO Kubernetes Operator is installed in the cluster
</file>

<file path="helm/osko/templates/prometheusrule_editor_role_binding.yaml">
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    {{ include "osko.labels" . | nindent 4 }}
  name: {{ .Release.Name }}-prometheusrule-editor-role
rules:
- apiGroups:
    - monitoring.coreos.com
  resources:
    - prometheusrules
  verbs:
    - get
    - list
    - watch
    - create
    - update
    - patch
    - delete
</file>

<file path="helm/osko/templates/prometheusrule_editor_role.yaml">
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    {{ include "osko.labels" . | nindent 4 }}
  name: prometheus-rule-editor-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: {{ .Release.Name }}-prometheusrule-editor-role
subjects:
  - kind: ServiceAccount
    name: {{ include "osko.fullname" . }}
    namespace: {{ .Release.Namespace }}
</file>

<file path="helm/osko/templates/serviceaccount.yaml">
{{- if .Values.serviceAccount.create -}}
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ include "osko.serviceAccountName" . }}
  labels:
    {{- include "osko.labels" . | nindent 4 }}
  {{- with .Values.serviceAccount.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
{{- end }}
</file>

<file path="helm/osko/.helmignore">
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*.orig
*~
# Various IDEs
.project
.idea/
*.tmproj
.vscode/
</file>

<file path="helm/osko-crds/charts/crds/Chart.yaml">
apiVersion: v2
name: crds
version: 0.0.0
</file>

<file path="helm/osko-crds/.helmignore">
# Patterns to ignore when building packages.
# This supports shell glob matching, relative path matching, and
# negation (prefixed with !). Only one pattern per line.
.DS_Store
# Common VCS dirs
.git/
.gitignore
.bzr/
.bzrignore
.hg/
.hgignore
.svn/
# Common backup files
*.swp
*.bak
*.tmp
*.orig
*~
# Various IDEs
.project
.idea/
*.tmproj
.vscode/
</file>

<file path="helm/osko-crds/Chart.yaml">
apiVersion: v2
type: application
version: 0.0.1
name: osko-crds
description: A Helm chart for osko CRDs
keywords:
  - osko
  - crds
appVersion: "0.0.1"
sources:
  - https://github.com/oskoperator/osko
maintainers:
  - name: fourstepper
    email: me@robinopletal.com
  - name: Hy3n4
    email: hy3nk4@gmail.com
home: https://github.com/oskoperator/osko

dependencies:
  - name: crds
    version: 0.0.0
</file>

<file path="internal/controller/openslo/sli_controller.go">
package controller
⋮----
import (
	"context"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
)
⋮----
"context"
apierrors "k8s.io/apimachinery/pkg/api/errors"
"k8s.io/apimachinery/pkg/runtime"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/log"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
⋮----
const (
	errGetSLI = "could not get SLI Object"
)
⋮----
// SLIReconciler reconciles a SLI object
type SLIReconciler struct {
	client.Client
	Scheme *runtime.Scheme
}
⋮----
//+kubebuilder:rbac:groups=openslo.com,resources=slis,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=openslo.com,resources=slis/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=openslo.com,resources=slis/finalizers,verbs=update
⋮----
func (r *SLIReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *SLIReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="internal/controller/osko/alertmanagerconfig_controller_test.go">
package osko
⋮----
import (
	"context"

	. "github.com/onsi/ginkgo/v2"
	. "github.com/onsi/gomega"
	"k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"

	oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
)
⋮----
"context"
⋮----
. "github.com/onsi/ginkgo/v2"
. "github.com/onsi/gomega"
"k8s.io/apimachinery/pkg/api/errors"
"k8s.io/apimachinery/pkg/types"
"sigs.k8s.io/controller-runtime/pkg/reconcile"
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
⋮----
var _ = Describe("AlertManagerConfig Controller", func() {
⋮----
const resourceName = "test-resource"
⋮----
Namespace: "default", // TODO(user):Modify as needed
⋮----
// TODO(user): Specify other spec details if needed.
⋮----
// TODO(user): Cleanup logic after each test, like removing the resource instance.
⋮----
// TODO(user): Add more specific assertions depending on your controller's reconciliation logic.
// Example: If you expect a certain status condition after reconciliation, verify it here.
</file>

<file path="ct.yaml">
# See https://github.com/helm/chart-testing#configuration
remote: origin
target-branch: main
chart-dirs:
  - charts
charts-repo: https://helm.osko.dev
validate-maintainers: false
</file>

<file path="DESIGN.md">
# osko - OpenSLO Kubernetes Operator

## Introduction

This operator aims to provide it's users with simple management of SLIs, SLOs, alerting rules and alerts routing via Kubernetes CRDs according to the (not only) the [OpenSLO](https://github.com/OpenSLO/OpenSLO) specification (currently `v1`).

## Goals

The goals of the operator are to take **inputs** in the form of metrics from supported datasources (`Mimir`, `Cortex` for a start) and produce **outputs** in the form of Prometheus `rules` in the form of the [`PrometheusRule`](https://prometheus-operator.dev/docs/operator/design/#prometheusrule) CRDs based on set `OpenSLO`s.

### Example inputs and outputs

#### Inputs

[kind: Datasource `spec.connectionDetails`](https://github.com/oskoperator/osko/blob/main/apis/openslo/v1/datasource_types.go#L11)

#### Outputs

[`PrometheusRule`](https://prometheus-operator.dev/docs/operator/design/#prometheusrule)

If the target system is unable to reconcile the created [`PrometheusRule`](https://prometheus-operator.dev/docs/operator/design/#prometheusrule)s on it's own (like [prometheus-operator](https://github.com/prometheus-operator/prometheus-operator)), we allow extending our operator with controllers that will be able to reconcile [`PrometheusRule`](https://prometheus-operator.dev/docs/operator/design/#prometheusrule)s against specific target systems (any arbitrary API, for example [`Cortex`s Ruler](https://cortexmetrics.io/docs/api/#ruler)).

## Non-Goals

- Support the full OpenSLO specification from the get-go, **if ever**
  - The goal here is to be _compatible_ with the OpenSLO spec, not necessarily fully implement it
  - MRs (PRs) are welcome for any missing functionality.

# Technical notes

- We should look into how to implement [Multiwindow, Multi-Burn-Rate Alerts](https://sre.google/workbook/alerting-on-slos/#6-multiwindow-multi-burn-rate-alerts) based on the OpenSLO spec

## Design

```mermaid
---
Title: OSKO Dependency Graph
---
flowchart LR;
subgraph userspace
sloObject(SLO)
sliObject(SLI)
dataSourceObject(DataSource)
end
subgraph controllerspace
prometheusRuleObject(PrometheusRule)
end

sloController(SLO Controller)
mimirRuleController(Mimir Rule Controller)
sliController(SLI Controller)
dataSourceController(DataSource Controller)

subgraph external
mimir[Mimir]
cortex[Cortex]
end

cortexRuleController(Optional: Cortex Rule Controller)
cortexRuleController --> |Watch| prometheusRuleObject
cortexRuleController --> |Updates| cortex

mimirRuleController --> |Watch| prometheusRuleObject
mimirRuleController --> |Updates| mimir

sloController --> |Own| sloObject
sloController --> |Watch| sliObject
sloController --> |Watch| dataSourceObject
sloController --> |Own| prometheusRuleObject

sliController --> |Own| sliObject
sliController --> |Watch| dataSourceObject

dataSourceController --> |Own| dataSourceObject

sloObject --> |Reference| sliObject
sliObject --> |Reference| dataSourceObject
%% reference slo -> datasource asi netreba, to bereme na zaklade SLIs ne? Dela to pak
%% hnusnej graf :D, kdyztak zkus odkomentovat
%%  sloObject --> |Reference| dataSourceObject
%%  prometheusRuleObject --> |Reference| dataSourceObject
```

## Resource Lifecycle

```mermaid
flowchart TD
    subgraph "User-Created Resources"
        DS[Datasource]
        SLI[SLI]
        SLO[SLO]
    end

    subgraph "Controller-Created Resources"
        PR[PrometheusRule]
        MR[MimirRule]
        AMC[AlertManagerConfig]
    end

    subgraph "External Systems"
        Mimir[(Mimir/Cortex)]
        AlertManager[(AlertManager)]
    end

    %% User resource relationships
    SLO -->|references| SLI
    SLO -->|references via annotation| DS
    SLI -->|references| DS

    %% Controller relationships
    SLOController[SLO Controller]
    MimirRuleController[MimirRule Controller]
    AMCController[AlertManagerConfig Controller]
    PRController[PrometheusRule Controller]

    %% Controller watches and creates
    SLOController -->|watches| SLO
    SLOController -->|creates/owns| PR
    SLOController -->|creates/owns| MR

    MimirRuleController -->|watches| MR
    MimirRuleController -->|submits rules to| Mimir

    AMCController -->|watches| AMC
    AMCController -->|submits config to| AlertManager

    PRController -->|watches| PR

    %% Resource lifecycle with finalizers
    SLO -->|deletes with finalizer| PR
    SLO -->|deletes with finalizer| MR

    MR -->|has finalizer| MimirRule_Finalizer[MimirRule Finalizer]
    MimirRule_Finalizer -->|cleanup rules in| Mimir

    AMC -->|has finalizer| AMC_Finalizer[AlertManagerConfig Finalizer]
    AMC_Finalizer -->|cleanup config in| AlertManager

    %% Status updates
    SLOController -->|updates status| SLO
    MimirRuleController -->|updates status| MR
    AMCController -->|updates status| AMC

    %% Legend
    classDef userResource fill:#b7e1cd,stroke:#82b366
    classDef controllerResource fill:#d0e0e3,stroke:#6c8ebf
    classDef controller fill:#ffe6cc,stroke:#d79b00
    classDef external fill:#f5f5f5,stroke:#666666
    classDef finalizer fill:#fff2cc,stroke:#d6b656

    class DS,SLI,SLO userResource
    class PR,MR,AMC controllerResource
    class SLOController,MimirRuleController,AMCController,PRController controller
    class Mimir,AlertManager external
    class MimirRule_Finalizer,AMC_Finalizer finalizer
```
</file>

<file path="Dockerfile">
# Build the manager binary
FROM golang:1.22 as builder
ARG TARGETOS
ARG TARGETARCH

WORKDIR /workspace
# Copy the Go Modules manifests
COPY go.mod go.mod
COPY go.sum go.sum
# cache deps before building and copying source so that we don't need to re-download as much
# and so that source changes don't invalidate our downloaded layer
RUN go mod download

# Copy the go source
COPY cmd/main.go cmd/main.go
COPY api/ api/
COPY internal/ internal/

# Build
# the GOARCH has not a default value to allow the binary be built according to the host where the command
# was called. For example, if we call make docker-build in a local env which has the Apple Silicon M1 SO
# the docker BUILDPLATFORM arg will be linux/arm64 when for Apple x86 it will be linux/amd64. Therefore,
# by leaving it empty we can ensure that the container and binary shipped on it will have the same platform.
RUN CGO_ENABLED=0 GOOS=${TARGETOS:-linux} GOARCH=${TARGETARCH} go build -a -o manager cmd/main.go

# Use distroless as minimal base image to package the manager binary
# Refer to https://github.com/GoogleContainerTools/distroless for more details
FROM gcr.io/distroless/static:nonroot
WORKDIR /
COPY --from=builder /workspace/manager .
USER 65532:65532

ENTRYPOINT ["/manager"]
</file>

<file path="PROJECT">
# Code generated by tool. DO NOT EDIT.
# This file is used to track the info used to scaffold your project
# and allow the plugins properly work.
# More info: https://book.kubebuilder.io/reference/project-config.html
domain: openslo
layout:
- go.kubebuilder.io/v4
multigroup: true
projectName: osko
repo: github.com/oskoperator/osko
resources:
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: openslo
  kind: Datasource
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: openslo
  kind: SLO
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: openslo
  kind: SLI
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: openslo
  kind: AlertPolicy
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: openslo
  kind: AlertCondition
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: openslo
  kind: AlertNotificationTarget
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  domain: openslo
  group: openslo
  kind: Service
  path: github.com/oskoperator/osko/api/openslo/v1
  version: v1
- controller: true
  domain: openslo
  group: osko
  kind: PrometheusRule
  version: v1alpha1
- controller: true
  domain: openslo
  group: monitoring.coreos.com
  kind: PrometheusRule
  version: v1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: osko
  kind: MimirRule
  path: github.com/oskoperator/osko/api/osko/v1alpha1
  version: v1alpha1
- api:
    crdVersion: v1
    namespaced: true
  controller: true
  domain: openslo
  group: osko
  kind: AlertManagerConfig
  path: github.com/oskoperator/osko/api/osko/v1alpha1
  version: v1alpha1
version: "3"
</file>

<file path="api/openslo/v1/zz_generated.deepcopy.go">
//go:build !ignore_autogenerated
⋮----
// Code generated by controller-gen. DO NOT EDIT.
⋮----
package v1
⋮----
import (
	"github.com/oskoperator/osko/api/osko/v1alpha1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
)
⋮----
"github.com/oskoperator/osko/api/osko/v1alpha1"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
runtime "k8s.io/apimachinery/pkg/runtime"
⋮----
// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *AlertCondition) DeepCopyInto(out *AlertCondition)
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertCondition.
func (in *AlertCondition) DeepCopy() *AlertCondition
⋮----
// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *AlertCondition) DeepCopyObject() runtime.Object
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertConditionList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertConditionSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertConditionStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertNotificationTarget.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertNotificationTargetList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertNotificationTargetSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertNotificationTargetStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertPolicy.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertPolicyCondition.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertPolicyList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertPolicyNotificationTarget.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertPolicySpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertPolicyStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new CalendarSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ConditionSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ConnectionDetails.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Datasource.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new DatasourceList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new DatasourceSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new DatasourceStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Indicator.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MetricSource.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MetricSourceSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MetricSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ObjectMetaOpenSLO.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ObjectivesSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new RatioMetricSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLI.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLIList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLISpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLIStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLO.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLOAlertPolicy.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLOList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLOSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SLOStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Service.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ServiceList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ServiceSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ServiceStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ThresholdMetricSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new TimeWindowSpec.
</file>

<file path="api/osko/v1alpha1/mimirrule_types.go">
package v1alpha1
⋮----
import (
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	"github.com/prometheus/common/model"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
"github.com/prometheus/common/model"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
// EDIT THIS FILE!  THIS IS SCAFFOLDING FOR YOU TO OWN!
// NOTE: json tags are required.  Any new fields you add must have json tags for the fields to be serialized.
⋮----
// MimirRuleSpec defines the desired state of MimirRule
type MimirRuleSpec struct {
	ConnectionDetails ConnectionDetails `json:"mimirConnectionDetails,omitempty"`
	// Groups is an example field of MimirRule. Edit mimirrule_types.go to remove/update
	Groups []RuleGroup `json:"groups"`
}
⋮----
// Groups is an example field of MimirRule. Edit mimirrule_types.go to remove/update
⋮----
// MimirRuleStatus defines the observed state of MimirRule
type MimirRuleStatus struct {
	Conditions         []metav1.Condition `json:"conditions,omitempty"`
	LastEvaluationTime metav1.Time        `json:"lastEvaluationTime,omitempty"`
	Ready              string             `json:"ready,omitempty"`
}
⋮----
type RuleGroup struct {
	Name                          string          `json:"name"`
	SourceTenants                 []string        `json:"source_tenants,omitempty"`
	Rules                         []Rule          `json:"rules"`
	Interval                      model.Duration  `json:"interval,omitempty"`
	EvaluationDelay               *model.Duration `json:"evaluation_delay,omitempty"`
	Limit                         int             `json:"limit,omitempty"`
	AlignEvaluationTimeOnInterval bool            `json:"align_evaluation_time_on_interval,omitempty"`
}
⋮----
type Rule struct {
	Record        string                 `json:"record,omitempty"`
	Alert         string                 `json:"alert,omitempty"`
	Expr          string                 `json:"expr"`
	For           *monitoringv1.Duration `json:"for,omitempty"`
	KeepFiringFor model.Duration         `json:"keep_firing_for,omitempty"`
	Labels        map[string]string      `json:"labels,omitempty"`
	Annotations   map[string]string      `json:"annotations,omitempty"`
}
⋮----
//+kubebuilder:object:root=true
//+kubebuilder:subresource:status
//+kubebuilder:printcolumn:name="Ready",type=string,JSONPath=.status.ready,description="The reason for the current status of the MimirRule resource"
//+kubebuilder:printcolumn:name="Age",type=date,JSONPath=`.metadata.creationTimestamp`
⋮----
// MimirRule is the Schema for the mimirrules API
type MimirRule struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   MimirRuleSpec   `json:"spec,omitempty"`
	Status MimirRuleStatus `json:"status,omitempty"`
}
⋮----
// MimirRuleList contains a list of MimirRule
type MimirRuleList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []MimirRule `json:"items"`
}
⋮----
func init()
</file>

<file path="config/rbac/role.yaml">
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: manager-role
rules:
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules/finalizers
  verbs:
  - update
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - openslo.com
  resources:
  - alertconditions
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertconditions/finalizers
  verbs:
  - update
- apiGroups:
  - openslo.com
  resources:
  - alertconditions/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets/finalizers
  verbs:
  - update
- apiGroups:
  - openslo.com
  resources:
  - alertnotificationtargets/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies/finalizers
  verbs:
  - update
- apiGroups:
  - openslo.com
  resources:
  - alertpolicies/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - openslo.com
  resources:
  - datasources
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - datasources/finalizers
  verbs:
  - update
- apiGroups:
  - openslo.com
  resources:
  - datasources/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - openslo.com
  resources:
  - slis
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - slis/finalizers
  verbs:
  - update
- apiGroups:
  - openslo.com
  resources:
  - slis/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - openslo.com
  resources:
  - slos
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - openslo.com
  resources:
  - slos/finalizers
  verbs:
  - update
- apiGroups:
  - openslo.com
  resources:
  - slos/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs/finalizers
  verbs:
  - update
- apiGroups:
  - osko.dev
  resources:
  - alertmanagerconfigs/status
  verbs:
  - get
  - patch
  - update
- apiGroups:
  - osko.dev
  resources:
  - mimirrules
  verbs:
  - create
  - delete
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - osko.dev
  resources:
  - mimirrules/finalizers
  verbs:
  - update
- apiGroups:
  - osko.dev
  resources:
  - mimirrules/status
  verbs:
  - get
  - patch
  - update
</file>

<file path="config/samples/config_secret.yaml">
apiVersion: v1
kind: Secret
metadata:
  labels:
    app.kubernetes.io/name: osko
    app.kubernetes.io/managed-by: kustomize
  name: alertmanagerconfig-sample
type: Opaque
stringData:
    alertmanager.yaml: |-
      route:
          receiver: 'default-receiver'
      receivers:
        - name: 'default-receiver'
          email_configs:
            - to: 'example@domain.com'
              from: 'alertmanager@domain.com'
              smarthost: 'smtp.domain.com:587'
              auth_username: 'alertmanager'
              auth_password: 'password'
              auth_identity: 'alertmanager@domain.com'
</file>

<file path="config/samples/kustomization.yaml">
## Append samples of your project ##
resources:
  - openslo_v1_datasource.yaml
  - openslo_v1_slo.yaml
  - config_secret.yaml
  - osko_v1alpha1_alertmanagerconfig.yaml
# +kubebuilder:scaffold:manifestskustomizesamples
</file>

<file path="config/samples/osko_v1alpha1_alertmanagerconfig.yaml">
apiVersion: osko.dev/v1alpha1
kind: AlertManagerConfig
metadata:
  annotations:
    osko.dev/datasourceRef: "mimir-infra-ds"
  labels:
    app.kubernetes.io/name: osko
    app.kubernetes.io/managed-by: kustomize
  name: alertmanagerconfig-sample
spec:
  secretRef:
    name: alertmanagerconfig-sample
</file>

<file path="devel/grafana-agent/configmap.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-agent-config
data:
  agent.river: |
    prometheus.remote_write "local" {
      endpoint {
        url = "http://mimir-service:9009/api/v1/push"
        headers = {
          "X-Scope-OrgID" = "monitoring",
        }
      }
    }
    prometheus.relabel "cluster" {
      rule {
        target_label = "cluster"
        replacement = "local"
      }
      forward_to = [
        prometheus.remote_write.local.receiver,
      ]
    }
    prometheus.scrape "static" {
      forward_to = [
        prometheus.relabel.cluster.receiver,
      ]
      targets = [
        {
          "__address__" = "mimir-service:9009",
        },
      ]
    }
</file>

<file path="devel/grafana-agent/deployment.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-agent
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana-agent
  template:
    metadata:
      labels:
        app: grafana-agent
    spec:
      containers:
        - name: grafana-agent
          image: grafana/agent:latest
          args:
            - "run"
            - "/etc/agent/agent.river"
            - "--storage.path=/tmp/agent"
            - "--server.http.listen-addr=127.0.0.1:12345"
            - "--server.http.ui-path-prefix=/"
          volumeMounts:
            - name: config-volume
              mountPath: /etc/agent
          env:
            - name: AGENT_MODE
              value: "flow"
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          ports:
            - containerPort: 12345
              name: http-agent

      volumes:
        - name: config-volume
          configMap:
            name: grafana-agent-config
</file>

<file path="devel/mimir/alertmanager-default-config.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: mimir-alertmanager-config
data:
  alertmanager-fallback-config.yaml: |
    route:
      group_wait: 0s
      receiver: empty-receiver
    receivers:
      # In this example we're not going to send any notification out of Alertmanager.
      - name: 'empty-receiver'
</file>

<file path="devel/mimir/deployment.yaml">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mimir
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mimir
  template:
    metadata:
      labels:
        app: mimir
    spec:
      containers:
        - name: mimir
          image: grafana/mimir:latest
          args: [
            "-config.file=/etc/mimir/config.yaml",
            "-target=all,alertmanager",
          ]
          ports:
            - containerPort: 9009
          volumeMounts:
            - name: config-volume
              mountPath: /etc/mimir
            - name: alertmanagerconfig-volume
              mountPath: /tmp/mimir/alertmanager-config/
      volumes:
        - name: config-volume
          configMap:
            name: mimir-config
        - name: alertmanagerconfig-volume
          configMap:
            name: mimir-alertmanager-config
</file>

<file path="helm/osko/templates/cluster-role-binding.yaml">
{{- if .Values.rbac.create -}}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    {{- include "osko.labels" . | nindent 4 }}
  name: {{ include "osko.fullname" . }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: {{ include "osko.fullname" . }}
subjects:
  - kind: ServiceAccount
    name: {{ include "osko.fullname" . }}
    namespace: {{ .Release.Namespace }}
{{- end }}
</file>

<file path="helm/osko/values.yaml">
# Default values for osko.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

installCRDs:
  prometheus-operator: false
  osko: true

replicaCount: 1

image:
  repository: ghcr.io/oskoperator/osko
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

rbac:
  create: true

podAnnotations: {}

securityContext:
  allowPrivilegeEscalation: false
  runAsNonRoot: true
  capabilities:
    drop:
      - ALL
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsUser: 1000

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
</file>

<file path="internal/controller/openslo/datasource_controller.go">
package controller
⋮----
import (
	"context"
	"fmt"
	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	"github.com/prometheus/client_golang/api"
	v1 "github.com/prometheus/client_golang/api/prometheus/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/client-go/tools/record"
	"net/http"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/log"
	"time"
)
⋮----
"context"
"fmt"
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
"github.com/prometheus/client_golang/api"
v1 "github.com/prometheus/client_golang/api/prometheus/v1"
apierrors "k8s.io/apimachinery/pkg/api/errors"
"k8s.io/apimachinery/pkg/runtime"
"k8s.io/client-go/tools/record"
"net/http"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/log"
"time"
⋮----
const (
	errGetDS     = "could not get Datasource"
	errConnectDS = "could not connect to Datasource"
	errQueryAPI  = "could not query API"
)
⋮----
// DatasourceReconciler reconciles a Datasource object
type DatasourceReconciler struct {
	client.Client
	Scheme   *runtime.Scheme
	Recorder record.EventRecorder
}
⋮----
type CustomRoundTripper struct {
	Transport http.RoundTripper
	TenantID  string
}
⋮----
//+kubebuilder:rbac:groups=openslo.com,resources=datasources,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=openslo.com,resources=datasources/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=openslo.com,resources=datasources/finalizers,verbs=update
⋮----
func (r *DatasourceReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// ignore Datasource deletion
⋮----
func (r *DatasourceReconciler) connectDatasource(ctx context.Context, ds *openslov1.Datasource) error
⋮----
func (c *CustomRoundTripper) RoundTrip(req *http.Request) (*http.Response, error)
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *DatasourceReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path=".github/workflows/publish-helm-chart.yaml">
name: Publish Chart
on:
  push:
    branches:
      - main
    paths:
      - 'helm/**'
    tags:
      - '*'

jobs:
  lint-chart:
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/')
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Helm
        uses: azure/setup-helm@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.0'
          check-latest: true

      - name: Cache dependecies
        uses: actions/cache@v4
        with:
          path: ~/.cache
          key: ${{ runner.os }}-helm-${{ hashFiles('**/Chart.yaml') }}
          restore-keys: |
            ${{ runner.os }}-helm-${{ hashFiles('**/Chart.yaml') }}

      - name: Create Kind Cluster
        uses: helm/kind-action@v1.8.0

      - name: Run Chart testing
        uses: helm/chart-testing-action@v2.1.0
        with:
          config: ct.yaml

  release:
    if: github.event_name == 'push' && startsWith(github.ref, 'refs/tags/')
    permissions:
      contents: write

    runs-on: ubuntu-latest
    needs:
      - lint-chart
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Configure Git
        run: |
          git config user.name "$GITHUB_ACTOR"
          git config user.email "$GITHUB_ACTOR@users.noreply.github.com"

      - name: Copy CRDs to it's chart
        run: |
          cp -rf config/crd/bases/* helm/osko-crds/charts/crds/templates/
          cp -rf config/crd/bases/* helm/osko/charts/crds/templates/

      - name: Install Helm
        uses: azure/setup-helm@v3

      - name: Add dependency chart repos
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts

      - name: Run chart-releaser
        uses: helm/chart-releaser-action@v1.6.0
        env:
          CR_TOKEN: "${{ secrets.GITHUB_TOKEN }}"
        with:
          charts_dir: helm
          skip_existing: false
          packages_with_index: true
</file>

<file path="devel/mimir/configmap.yaml">
apiVersion: v1
kind: ConfigMap
metadata:
  name: mimir-config
data:
  config.yaml: |
    multitenancy_enabled: true
    tenant_federation:
      enabled: true

    blocks_storage:
      backend: filesystem
      bucket_store:
        sync_dir: /tmp/mimir/tsdb-sync
      filesystem:
        dir: /tmp/mimir/data/tsdb
      tsdb:
        dir: /tmp/mimir/tsdb

    alertmanager:
      data_dir: /tmp/mimir/alertmanager
      enable_api: true
      fallback_config_file: /tmp/mimir/alertmanager-config/alertmanager-fallback-config.yaml

    alertmanager_storage:
      backend: filesystem
      filesystem:
        dir: /tmp/mimir/alert-store

    compactor:
      data_dir: /tmp/mimir/compactor
      sharding_ring:
        kvstore:
          store: memberlist

    distributor:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist

    ingester:
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: memberlist
        replication_factor: 1

    ruler:
      alertmanager_url: http://mimir:9009/alertmanager
      enable_api: true
      rule_path: /tmp/mimir/ruler
      tenant_federation:
        enabled: true

    ruler_storage:
      backend: filesystem
      filesystem:
        dir: /tmp/mimir/rules

    server:
      http_listen_port: 9009
      log_level: info

    store_gateway:
      sharding_ring:
        replication_factor: 1
</file>

<file path="docs/labels-and-annotations.md">
# Labels and annotations

OSKO resources often use labels and annotations for configuring different behaviors for different
OpenSLO types. This is the documentation of the available labels and annotations that you can use
and what they are used for.

## Labels

### `label.osko.dev/<key>`

Enables labeling of Prometheus recording and alerting rules, for example for alert routing purposes.

```yaml
label.osko.dev/team: "infrastructure"
```

## Annotations

### `osko.dev/datasourceRef`

Configures which Datasource to use in an SLO definition.

Accepts a name of the Datasource as string.

```yaml
osko.dev/datasourceRef: "mimir-infra-ds"
```

### `osko.dev/baseWindow`

Configures the base window for an individual SLO (instead of the default of "5m" specified in the config).

Accepts a string in the [time.Duration](https://pkg.go.dev/time#Duration) format.

```yaml
osko.dev/baseWindow: "30m"
```

### `osko.dev/magicAlerting`

Configures whether OSKO creates multiwindow, multi-burn-rate alerts for the SLO, automagically.

Accepts the string "true" as the only valid input.

```yaml
osko.dev/magicAlerting: "true"
```
</file>

<file path="helm/osko/templates/cluster-role.yaml">
{{- if .Values.rbac.create -}}
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: {{ include "osko.fullname" . }}
rules:
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - prometheusrules
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - prometheusrules/finalizers
    verbs:
      - update
  - apiGroups:
      - monitoring.coreos.com
    resources:
      - prometheusrules/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - openslo.com
    resources:
      - alertconditions
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - openslo.com
    resources:
      - alertconditions/finalizers
    verbs:
      - update
  - apiGroups:
      - openslo.com
    resources:
      - alertconditions/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - openslo.com
    resources:
      - alertnotificationtargets
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - openslo.com
    resources:
      - alertnotificationtargets/finalizers
    verbs:
      - update
  - apiGroups:
      - openslo.com
    resources:
      - alertnotificationtargets/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - openslo.com
    resources:
      - alertpolicies
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - openslo.com
    resources:
      - alertpolicies/finalizers
    verbs:
      - update
  - apiGroups:
      - openslo.com
    resources:
      - alertpolicies/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - openslo.com
    resources:
      - datasources
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - openslo.com
    resources:
      - datasources/finalizers
    verbs:
      - update
  - apiGroups:
      - openslo.com
    resources:
      - datasources/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - openslo.com
    resources:
      - slis
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - openslo.com
    resources:
      - slis/finalizers
    verbs:
      - update
  - apiGroups:
      - openslo.com
    resources:
      - slis/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - openslo.com
    resources:
      - slos
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - openslo.com
    resources:
      - slos/finalizers
    verbs:
      - update
  - apiGroups:
      - openslo.com
    resources:
      - slos/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - osko.dev
    resources:
      - mimirrules
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - osko.dev
    resources:
      - mimirrules/finalizers
    verbs:
      - update
  - apiGroups:
      - osko.dev
    resources:
      - mimirrules/status
    verbs:
      - get
      - patch
      - update
  - apiGroups:
      - osko.dev
    resources:
      - alertmanagerconfigs
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - osko.dev
    resources:
      - alertmanagerconfigs/status
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
      - watch
  {{- end }}
</file>

<file path="internal/config/config.go">
package config
⋮----
import (
	"time"
)
⋮----
"time"
⋮----
var Cfg Config
⋮----
func NewConfig()
⋮----
// AlertSeverities:   AlertSeveritiesByTool(alertingTool), // I wouldn't default to opsgenie here, maybe better to default to custom and error on startup if no custom variables or valid tool is selected
</file>

<file path="internal/config/types.go">
package config
⋮----
import (
	"time"
)
⋮----
"time"
⋮----
type Config struct {
	MimirRuleRequeuePeriod time.Duration
	AlertingBurnRates      AlertingBurnRates
	DefaultBaseWindow      time.Duration
	AlertingTool           string
	AlertSeverities        AlertSeverities
}
⋮----
type AlertingBurnRates struct {
	PageShortWindow   float64
	PageLongWindow    float64
	TicketShortWindow float64
	TicketLongWindow  float64
}
⋮----
type AlertToolConfig struct {
	Tool       string
	Severities map[string]string
}
⋮----
type SREAlertSeverity string
⋮----
const (
	PageCritical SREAlertSeverity = "page_critical"
	PageHigh     SREAlertSeverity = "page_high"
	TicketHigh   SREAlertSeverity = "ticket_high"
	TicketMedium SREAlertSeverity = "ticket_medium"
)
⋮----
type AlertToolSeverityMap map[SREAlertSeverity]string
⋮----
type AlertSeverities struct {
	Critical string
	HighFast string
	HighSlow string
	Low      string
	Tool     string
}
⋮----
func (m AlertToolSeverityMap) GetSeverity(sreSeverity SREAlertSeverity) string
⋮----
return m[TicketMedium] // default to lowest severity
</file>

<file path="internal/config/utils.go">
package config
⋮----
import (
	"os"
	"strconv"
	"time"
)
⋮----
"os"
"strconv"
"time"
⋮----
// GetEnv Helper function to read an environment variable or return a default value
func GetEnv(key, defaultValue string) string
⋮----
// GetEnvAsInt Helper function to read an environment variable as an integer or return a default value
func GetEnvAsInt(key string, defaultValue int) int
⋮----
// GetEnvAsFloat64 Helper function to read an environment variable as a float64 or return a default value
func GetEnvAsFloat64(key string, defaultValue float64) float64
⋮----
// GetEnvAsDuration Helper function to read an environment variable as a time.Duration or return a default value
func GetEnvAsDuration(key string, defaultValue time.Duration) time.Duration
⋮----
func AlertSeveritiesByTool(tool string) AlertToolSeverityMap
</file>

<file path="config/samples/openslo_v1_datasource.yaml">
apiVersion: openslo.com/v1
kind: Datasource
metadata:
  labels:
    app.kubernetes.io/name: mimir-infra-ds
  name: mimir-infra-ds
spec:
  description: Mimir Datasource for logging tenant
  type: mimir
  connectionDetails:
    address: http://localhost:9009/
    sourceTenants:
      - gatekeeper-system
      - monitoring
    targetTenant: monitoring
</file>

<file path="api/osko/v1alpha1/alertmanagerconfig_types.go">
package v1alpha1
⋮----
import (
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
type SecretRef struct {
	Name      string `json:"name"`
	Namespace string `json:"namespace,omitempty"`
}
⋮----
// AlertManagerConfigSpec defines the desired state of AlertManagerConfig
type AlertManagerConfigSpec struct {
	SecretRef SecretRef `json:"secretRef,omitempty"`
}
⋮----
// AlertManagerConfigStatus defines the observed state of AlertManagerConfig
type AlertManagerConfigStatus struct {
	Conditions         []metav1.Condition `json:"conditions,omitempty"`
	LastEvaluationTime metav1.Time        `json:"lastEvaluationTime,omitempty"`
	Ready              string             `json:"ready,omitempty"`
}
⋮----
// +kubebuilder:object:root=true
// +kubebuilder:subresource:status
// +kubebuilder:printcolumn:name="Ready",type=string,JSONPath=.status.ready,description="The reason for the current status of the AlertmanagerConfig resource"
// +kubebuilder:printcolumn:name="Age",type=date,JSONPath=.metadata.creationTimestamp,description="The time when the AlertmanagerConfig resource was created"
⋮----
// AlertManagerConfig is the Schema for the alertmanagerconfigs API
type AlertManagerConfig struct {
	metav1.TypeMeta   `json:",inline"`
	metav1.ObjectMeta `json:"metadata,omitempty"`

	Spec   AlertManagerConfigSpec   `json:"spec,omitempty"`
	Status AlertManagerConfigStatus `json:"status,omitempty"`
}
⋮----
// AlertManagerConfigList contains a list of AlertManagerConfig
type AlertManagerConfigList struct {
	metav1.TypeMeta `json:",inline"`
	metav1.ListMeta `json:"metadata,omitempty"`
	Items           []AlertManagerConfig `json:"items"`
}
⋮----
func init()
</file>

<file path="cmd/main.go">
package main
⋮----
import (
	"flag"
	"os"

	monitoringcoreoscom "github.com/oskoperator/osko/internal/controller/monitoring.coreos.com"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	monitoringv1alpha1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1alpha1"

	"github.com/oskoperator/osko/internal/config"

	// Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.)
⋮----
"flag"
"os"
⋮----
monitoringcoreoscom "github.com/oskoperator/osko/internal/controller/monitoring.coreos.com"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
monitoringv1alpha1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1alpha1"
⋮----
"github.com/oskoperator/osko/internal/config"
⋮----
// Import all Kubernetes client auth plugins (e.g. Azure, GCP, OIDC, etc.)
// to ensure that exec-entrypoint and run can make use of them.
_ "k8s.io/client-go/plugin/pkg/client/auth"
⋮----
"k8s.io/apimachinery/pkg/runtime"
utilruntime "k8s.io/apimachinery/pkg/util/runtime"
clientgoscheme "k8s.io/client-go/kubernetes/scheme"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/healthz"
"sigs.k8s.io/controller-runtime/pkg/log/zap"
metricsserver "sigs.k8s.io/controller-runtime/pkg/metrics/server"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
⋮----
openslov1controller "github.com/oskoperator/osko/internal/controller/openslo"
oskocontroller "github.com/oskoperator/osko/internal/controller/osko"
//+kubebuilder:scaffold:imports
⋮----
var (
	scheme   = runtime.NewScheme()
⋮----
func init()
⋮----
//+kubebuilder:scaffold:scheme
⋮----
func main()
⋮----
var metricsAddr string
var enableLeaderElection bool
var probeAddr string
⋮----
// LeaderElectionReleaseOnCancel defines if the leader should step down voluntarily
// when the Manager ends. This requires the binary to immediately end when the
// Manager is stopped, otherwise, this setting is unsafe. Setting this significantly
// speeds up voluntary leader transitions as the new leader don't have to wait
// LeaseDuration time first.
//
// In the default scaffold provided, the program ends immediately after
// the manager stops, so would be fine to enable this option. However,
// if you are doing or is intended to do any operation such as perform cleanups
// after the manager stops then its usage might be unsafe.
// LeaderElectionReleaseOnCancel: true,
⋮----
//+kubebuilder:scaffold:builder
</file>

<file path="internal/controller/monitoring.coreos.com/prometheusrule_controller.go">
package monitoringcoreoscom
⋮----
import (
	"context"
	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	"github.com/oskoperator/osko/internal/helpers"
	"github.com/oskoperator/osko/internal/utils"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/record"
	"reflect"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
)
⋮----
"context"
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
"github.com/oskoperator/osko/internal/helpers"
"github.com/oskoperator/osko/internal/utils"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
apierrors "k8s.io/apimachinery/pkg/api/errors"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
"k8s.io/apimachinery/pkg/runtime"
"k8s.io/apimachinery/pkg/types"
"k8s.io/client-go/tools/record"
"reflect"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/handler"
ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
⋮----
const (
	objectiveRef = ".metaData.ownerReferences.name"
	errGetSLI    = "could not get SLI Object"
)
⋮----
// PrometheusRuleReconciler reconciles a PrometheusRule object
type PrometheusRuleReconciler struct {
	client.Client
	Scheme   *runtime.Scheme
	Recorder record.EventRecorder
}
⋮----
// +kubebuilder:rbac:groups=monitoring.coreos.com,resources=prometheusrules,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=monitoring.coreos.com,resources=prometheusrules/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=monitoring.coreos.com,resources=prometheusrules/finalizers,verbs=update
// +kubebuilder:rbac:groups=core,resources=events,verbs=create;patch
⋮----
func (r *PrometheusRuleReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// check if the PrometheusRule is owned by an SLO
⋮----
// if not, check if we are supposed to manage it or not
⋮----
// Get SLI from SLO's ref
⋮----
// Update PrometheusRule
// This is the main logic for the PrometheusRule update
// Here we should take the existing PrometheusRule and update it with the new one
⋮----
// has to be the same as for previous object, otherwise it will not be updated and throw an error
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *PrometheusRuleReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="internal/utils/common_utils.go">
package utils
⋮----
import (
	"context"
	"fmt"
	"github.com/go-logr/logr"
	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
	"reflect"
	"sigs.k8s.io/controller-runtime/pkg/client"
	ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
	"time"
)
⋮----
"context"
"fmt"
"github.com/go-logr/logr"
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
"k8s.io/apimachinery/pkg/util/intstr"
"reflect"
"sigs.k8s.io/controller-runtime/pkg/client"
ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
"time"
⋮----
type MetricLabel struct {
	Slo        *openslov1.SLO
	Sli        *openslov1.SLI
	TimeWindow string
	Labels     map[string]string
}
⋮----
type Rule struct {
	Sli                 *openslov1.SLI
	Slo                 *openslov1.SLO
	BaseRule            *monitoringv1.Rule
	RuleType            string
	Record              string
	Expr                string
	RateWindow          string
	TimeWindow          string
	SupportiveRule      *Rule
	MetricLabelCompiler *MetricLabel
}
⋮----
type BudgetRule struct {
	Record           string
	Sli              *openslov1.SLI
	Slo              *openslov1.SLO
	TotalRuleConfig  *Rule
	BadRuleConfig    *Rule
	GoodRuleConfig   *Rule
	TargetRuleConfig *Rule
}
⋮----
type DataSourceConfig struct {
	DataSource *openslov1.Datasource
}
⋮----
const (
	RecordPrefix    = "osko"
	TypeTotal       = "total"
	TypeBad         = "bad"
	TypeGood        = "good"
	TypeMeasurement = "sli_measurement"
	ExprFmt         = "sum(increase(%s[%s]))"
⋮----
// UpdateCondition checks if the condition of the given type is already in the slice
// if the condition already exists and has the same status, return the unmodified conditions
// if the condition exists and has a different status, remove it and add the new one
// if the condition does not exist, add it
func updateCondition(conditions []metav1.Condition, newCondition metav1.Condition) []metav1.Condition
⋮----
var existingCondition metav1.Condition
⋮----
// Filter the existing condition (if it exists)
var updatedConditions []metav1.Condition
⋮----
// Append the new condition
⋮----
func UpdateStatus(ctx context.Context, obj client.Object, r client.Client, conditionType string, status metav1.ConditionStatus, message string) error
⋮----
// Update the conditions based on provided arguments
⋮----
func (m MetricLabel) NewMetricLabelCompiler(rule *monitoringv1.Rule, window string) string
⋮----
func (m MetricLabel) NewMetricLabelGenerator() map[string]string
⋮----
func (c Rule) getFieldsByType() (string, error)
⋮----
func (c Rule) NewRatioRule(window string) (*monitoringv1.Rule, *monitoringv1.Rule)
⋮----
//
⋮----
func (c Rule) NewSupportiveRule(baseRule monitoringv1.Rule) (rule monitoringv1.Rule)
⋮----
func (c Rule) NewTargetRule() (rule monitoringv1.Rule)
⋮----
func (b BudgetRule) NewBudgetRule() (budgetRule monitoringv1.Rule, sliMeasurement monitoringv1.Rule)
⋮----
//sliMeasurement = createSLIMeasurement(gbRule, b.TotalRuleConfig)
⋮----
func getRelevantRule(b BudgetRule, goodRuleSpec, sloIndicatorSpec string, log logr.Logger) *Rule
⋮----
func CreateSLIMeasurement(m MetricLabel, goodRule monitoringv1.Rule, totalRule monitoringv1.Rule) monitoringv1.Rule
⋮----
func createBudgetRule(b BudgetRule, gbRule *Rule) monitoringv1.Rule
⋮----
func (d DataSourceConfig) ParseTenantAnnotation() (tenants []string)
</file>

<file path="Makefile">
# Image URL to use all building/pushing image targets
IMG ?= localhost:5000/slo-kubernetes-operator/slo-kubernetes-operator/operator:latest
# ENVTEST_K8S_VERSION refers to the version of kubebuilder assets to be downloaded by envtest binary.
ENVTEST_K8S_VERSION = 1.27.1

# Get the currently used golang install path (in GOPATH/bin, unless GOBIN is set)
ifeq (,$(shell go env GOBIN))
GOBIN=$(shell go env GOPATH)/bin
else
GOBIN=$(shell go env GOBIN)
endif

# CONTAINER_TOOL defines the container tool to be used for building images.
# Be aware that the target commands are only tested with Docker which is
# scaffolded by default. However, you might want to replace it to use other
# tools. (i.e. podman)
CONTAINER_TOOL ?= docker

# Setting SHELL to bash allows bash commands to be executed by recipes.
# Options are set to exit when a recipe line exits non-zero or a piped command fails.
SHELL = /usr/bin/env bash -o pipefail
.SHELLFLAGS = -ec

.PHONY: all
all: build

##@ General

# The help target prints out all targets with their descriptions organized
# beneath their categories. The categories are represented by '##@' and the
# target descriptions by '##'. The awk commands is responsible for reading the
# entire set of makefiles included in this invocation, looking for lines of the
# file as xyz: ## something, and then pretty-format the target and help. Then,
# if there's a line with ##@ something, that gets pretty-printed as a category.
# More info on the usage of ANSI control characters for terminal formatting:
# https://en.wikipedia.org/wiki/ANSI_escape_code#SGR_parameters
# More info on the awk command:
# http://linuxcommand.org/lc3_adv_awk.php

.PHONY: help
help: ## Display this help.
	@awk 'BEGIN {FS = ":.*##"; printf "\nUsage:\n  make \033[36m<target>\033[0m\n"} /^[a-zA-Z_0-9-]+:.*?##/ { printf "  \033[36m%-15s\033[0m %s\n", $$1, $$2 } /^##@/ { printf "\n\033[1m%s\033[0m\n", substr($$0, 5) } ' $(MAKEFILE_LIST)

##@ Development

.PHONY: manifests
manifests: controller-gen ## Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.
	$(CONTROLLER_GEN) rbac:roleName=manager-role crd:generateEmbeddedObjectMeta=true webhook paths="./..." output:crd:artifacts:config=config/crd/bases

.PHONY: generate
generate: controller-gen ## Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.
	$(CONTROLLER_GEN) object paths="./..."

.PHONY: fmt
fmt: ## Run go fmt against code.
	go fmt ./...

.PHONY: vet
vet: ## Run go vet against code.
	go vet ./...

.PHONY: test
test: manifests generate fmt vet envtest ## Run tests.
	KUBEBUILDER_ASSETS="$(shell $(ENVTEST) use $(ENVTEST_K8S_VERSION) --bin-dir $(LOCALBIN) -p path)" go test ./... -coverprofile cover.out

##@ Build

.PHONY: build
build: manifests generate fmt vet ## Build manager binary.
	go build -o bin/manager cmd/main.go

.PHONY: run
run: manifests generate fmt vet ## Run a controller from your host.
	go run ./cmd/main.go

.PHONY: run-pretty-debug
run-pretty-debug: manifests generate fmt vet ## Run a controller from your host with pretty debug output.
	go run ./cmd/main.go --zap-log-level=debug 2>&1 | zap-pretty

# If you wish built the manager image targeting other platforms you can use the --platform flag.
# (i.e. docker build --platform linux/arm64 ). However, you must enable docker buildKit for it.
# More info: https://docs.docker.com/develop/develop-images/build_enhancements/
.PHONY: docker-build
docker-build: test ## Build docker image with the manager.
	$(CONTAINER_TOOL) build -t ${IMG} .

.PHONY: docker-push
docker-push: ## Push docker image with the manager.
	$(CONTAINER_TOOL) push ${IMG}

# PLATFORMS defines the target platforms for  the manager image be build to provide support to multiple
# architectures. (i.e. make docker-buildx IMG=myregistry/mypoperator:0.0.1). To use this option you need to:
# - able to use docker buildx . More info: https://docs.docker.com/build/buildx/
# - have enable BuildKit, More info: https://docs.docker.com/develop/develop-images/build_enhancements/
# - be able to push the image for your registry (i.e. if you do not inform a valid value via IMG=<myregistry/image:<tag>> then the export will fail)
# To properly provided solutions that supports more than one platform you should use this option.
PLATFORMS ?= linux/arm64,linux/amd64,linux/s390x,linux/ppc64le
.PHONY: docker-buildx
docker-buildx: test ## Build and push docker image for the manager for cross-platform support
	# copy existing Dockerfile and insert --platform=${BUILDPLATFORM} into Dockerfile.cross, and preserve the original Dockerfile
	sed -e '1 s/\(^FROM\)/FROM --platform=\$$\{BUILDPLATFORM\}/; t' -e ' 1,// s//FROM --platform=\$$\{BUILDPLATFORM\}/' Dockerfile > Dockerfile.cross
	- $(CONTAINER_TOOL) buildx create --name project-v3-builder
	$(CONTAINER_TOOL) buildx use project-v3-builder
	- $(CONTAINER_TOOL) buildx build --push --platform=$(PLATFORMS) --tag ${IMG} -f Dockerfile.cross .
	- $(CONTAINER_TOOL) buildx rm project-v3-builder
	rm Dockerfile.cross

##@ Deployment

ifndef ignore-not-found
  ignore-not-found = false
endif

.PHONY: install
install: manifests kustomize ## Install CRDs into the K8s cluster specified in ~/.kube/config.
	$(KUSTOMIZE) build config/crd | $(KUBECTL) apply -f -

.PHONY: uninstall
uninstall: manifests kustomize ## Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
	$(KUSTOMIZE) build config/crd | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -

.PHONY: deploy
deploy: manifests kustomize ## Deploy controller to the K8s cluster specified in ~/.kube/config.
	cd config/manager && $(KUSTOMIZE) edit set image controller=${IMG}
	$(KUSTOMIZE) build config/default | $(KUBECTL) apply -f -

.PHONY: undeploy
undeploy: ## Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.
	$(KUSTOMIZE) build config/default | $(KUBECTL) delete --ignore-not-found=$(ignore-not-found) -f -

##@ Build Dependencies

## Location to install dependencies to
LOCALBIN ?= $(shell pwd)/bin
$(LOCALBIN):
	mkdir -p $(LOCALBIN)

## Tool Binaries
KUBECTL ?= kubectl
KUSTOMIZE ?= $(LOCALBIN)/kustomize
CONTROLLER_GEN ?= $(LOCALBIN)/controller-gen
ENVTEST ?= $(LOCALBIN)/setup-envtest

## Tool Versions
KUSTOMIZE_VERSION ?= v5.0.1
CONTROLLER_TOOLS_VERSION ?= v0.14.0

.PHONY: kustomize
kustomize: $(KUSTOMIZE) ## Download kustomize locally if necessary. If wrong version is installed, it will be removed before downloading.
$(KUSTOMIZE): $(LOCALBIN)
	@if test -x $(LOCALBIN)/kustomize && ! $(LOCALBIN)/kustomize version | grep -q $(KUSTOMIZE_VERSION); then \
		echo "$(LOCALBIN)/kustomize version is not expected $(KUSTOMIZE_VERSION). Removing it before installing."; \
		rm -rf $(LOCALBIN)/kustomize; \
	fi
	test -s $(LOCALBIN)/kustomize || GOBIN=$(LOCALBIN) GO111MODULE=on go install sigs.k8s.io/kustomize/kustomize/v5@$(KUSTOMIZE_VERSION)

.PHONY: controller-gen
controller-gen: $(CONTROLLER_GEN) ## Download controller-gen locally if necessary. If wrong version is installed, it will be overwritten.
$(CONTROLLER_GEN): $(LOCALBIN)
	test -s $(LOCALBIN)/controller-gen && $(LOCALBIN)/controller-gen --version | grep -q $(CONTROLLER_TOOLS_VERSION) || \
	GOBIN=$(LOCALBIN) go install sigs.k8s.io/controller-tools/cmd/controller-gen@$(CONTROLLER_TOOLS_VERSION)

.PHONY: envtest
envtest: $(ENVTEST) ## Download envtest-setup locally if necessary.
$(ENVTEST): $(LOCALBIN)
	test -s $(LOCALBIN)/setup-envtest || GOBIN=$(LOCALBIN) go install sigs.k8s.io/controller-runtime/tools/setup-envtest@latest

.PHONY: deploydev
deploydev:
	@$(KUBECTL) apply -k devel/
	@echo "Waiting for services to come online for the port-forwards..."
	@until [ "$$($(KUBECTL) get pods -l app=grafana -o jsonpath='{.items}')}" != "[]" ] && \
			[ "$$($(KUBECTL) get pods -l app=grafana -o jsonpath='{.items[0].status.containerStatuses[0].ready}')" == "true" ]; do \
			echo "Waiting for Grafana to be ready..." && sleep 2; \
		done
	@until [ "$$($(KUBECTL) get pods -l app=mimir -o jsonpath='{.items}')}" != "[]" ] && \
		[ "$$($(KUBECTL) get pods -l app=mimir -o jsonpath='{.items[0].status.containerStatuses[0].ready}')" == "true" ]; do \
		echo "Waiting for Mimir to be ready..." && sleep 2; \
	done
	@echo "Services are ready. Setting up port-forwards..."
	@$(KUBECTL) port-forward svc/grafana 3000:3000 > /dev/null 2>&1 &
	@$(KUBECTL) port-forward svc/mimir-service 9009:9009 >/dev/null 2>&1 &
	@echo "Port-forwards activated. Reach Grafana on port 3000 and Mimir on port 9009."
	@echo "Enjoy!"

.PHONY: undeploydev
undeploydev:
	@$(KUBECTL) delete -R -f devel/ 2>/dev/null || true && echo "All resources are already deleted, skipping..."
	@for pf in mimir-service grafana; do (pkill -f "port-forward svc/$$pf" || true && echo "The $$pf port-forward process is now killed..."); done
	@echo "All done. Goodbye!"
</file>

<file path="README.md">
# osko - OpenSLO Kubernetes Operator

This operator aims to provide it's users with simple management of SLIs, SLOs, alerting rules and alerts routing via Kubernetes CRDs according to the [OpenSLO](https://github.com/OpenSLO/OpenSLO) specification (currently `v1`).

See the [design document](DESIGN.md) for more details on what `osko` aims to do.

## Here be dragons!

`osko` is in very active development, hardly functional and definitely not stable. Until a `v1` release comes around, use at your own risk.

## Test It Out

1. You’ll need a Kubernetes cluster to run `osko`. You can use [KIND](https://sigs.k8s.io/kind) to get a local cluster for testing, or run against a remote cluster.
   - refer to the [Installation and usage](https://github.com/kubernetes-sigs/kind#installation-and-usage) section of the [KIND](https://sigs.k8s.io/kind) README to use KIND.

**Note:** Your controller will automatically use the current context in your kubeconfig file (i.e. whatever cluster `kubectl cluster-info` shows).

2. Install the CRDs into the cluster:

```sh
make install
```

3. We also depend on Prometheus Operator CRDs (`monitoring.coreos.com` API group). Let's install that to our local cluster now:

```sh
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus-operator-crds prometheus-community/prometheus-operator-crds
```

4. Install sample CRDs into the cluster, so `osko` has resources to work with:

```sh
kubectl apply -k config/samples/
```

5. Run your controller (this will run in the foreground, so switch to a new terminal if you want to leave it running):

```sh
make run
```

**NOTE:** You can also run this in one step by running: `make install run`

#### Modifying the API definitions

If you are editing the API definitions, generate the manifests such as CRs or CRDs using:

```sh
make manifests
```

**NOTE:** Run `make --help` for more information on all potential `make` targets

More information can be found via the [Kubebuilder Documentation](https://book.kubebuilder.io/introduction.html)

## Contributing

Feel free to open an issue or a pull (merge) request if
you would like to see a particular feature implemented after reading the below requirements:

- Please sign your commits off using the `-s` flag during `git commit` after reading the
  project's [DCO](DCO).
- It would be greatly appreciated if you tried using
  [Conventional Commits](https://www.conventionalcommits.org/en/v1.0.0/) for the commit style.

## License

For license, see the [LICENSE](LICENSE) file in the root of this repository.

## Community

If you have any questions or need general advice or help, feel free to join the
[#osko channel on the OpenSLO Slack](https://openslo.slack.com/archives/C06T64CP5DK)

## Sponsors

<img src="assets/HG Logo_Heureka Group Color.png" width="33%">
</file>

<file path="config/samples/openslo_v1_slo.yaml">
apiVersion: openslo.com/v1
kind: SLO
metadata:
  name: mimir-ingestion-latency
  labels:
    label.osko.dev/team: "infra"
    label.osko.dev/system: "monitoring"
    label.osko.dev/domain: "observability"
    label.osko.dev/service: "mimir"
  annotations:
    osko.dev/datasourceRef: "mimir-infra-ds"
    osko.dev/magicAlerting: "true"
spec:
  budgetingMethod: Occurrences
  description: 95% of all queries should have a latency of less than 300 milliseconds
  indicator:
    metadata:
      name: distributor-query-success-latency
    spec:
      description: 95% of all queries should have a latency of less than 500 milliseconds
      ratioMetric:
        good:
          metricSource:
            metricSourceRef: mimir-infra-ds
            type: Mimir
            spec:
              query: cortex_distributor_query_duration_seconds_bucket{le="0.5", method="Distributor.QueryStream", status_code="200"}
        total:
          metricSource:
            metricSourceRef: mimir-infra-ds
            type: Mimir
            spec:
              query: cortex_distributor_query_duration_seconds_count{method="Distributor.QueryStream"}
  objectives:
    - target: "0.99"
  service: mimir
  timeWindow:
    - duration: 28d
      isRolling: true
</file>

<file path="internal/controller/osko/alertmanagerconfig_controller.go">
package osko
⋮----
import (
	"context"
	"fmt"
	"time"

	"github.com/oskoperator/osko/internal/utils"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/types"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	"sigs.k8s.io/controller-runtime/pkg/handler"

	mimirclient "github.com/grafana/mimir/pkg/mimirtool/client"
	"github.com/oskoperator/osko/internal/helpers"
	corev1 "k8s.io/api/core/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/client-go/tools/record"
	ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)
⋮----
"context"
"fmt"
"time"
⋮----
"github.com/oskoperator/osko/internal/utils"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
"k8s.io/apimachinery/pkg/types"
"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
"sigs.k8s.io/controller-runtime/pkg/handler"
⋮----
mimirclient "github.com/grafana/mimir/pkg/mimirtool/client"
"github.com/oskoperator/osko/internal/helpers"
corev1 "k8s.io/api/core/v1"
apierrors "k8s.io/apimachinery/pkg/api/errors"
"k8s.io/client-go/tools/record"
ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
"sigs.k8s.io/controller-runtime/pkg/reconcile"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
"k8s.io/apimachinery/pkg/runtime"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
⋮----
// AlertManagerConfigReconciler reconciles a AlertManagerConfig object
type AlertManagerConfigReconciler struct {
	client.Client
	Scheme      *runtime.Scheme
	Recorder    record.EventRecorder
	MimirClient *mimirclient.MimirClient
}
⋮----
const (
	errGetAMC                   = "Failed to get AlertmanagerConfig"
	alertmanagerConfigFinalizer = "alertmanagerconfig.osko.dev/finalizer"
)
⋮----
// +kubebuilder:rbac:groups=osko.dev,resources=alertmanagerconfigs,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=osko.dev,resources=alertmanagerconfigs/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=osko.dev,resources=alertmanagerconfigs/finalizers,verbs=update
⋮----
// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
// TODO(user): Modify the Reconcile function to compare the state specified by
// the AlertManagerConfig object against the actual cluster state, and then
// perform operations to make the cluster state reflect the state specified by
// the user.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.18.2/pkg/reconcile
func (r *AlertManagerConfigReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// Get DS from AMC's ref
⋮----
//amc.Status.Ready = "False"
⋮----
func (r *AlertManagerConfigReconciler) findObjectsForSecret() func(ctx context.Context, a client.Object) []reconcile.Request
⋮----
func (r *AlertManagerConfigReconciler) deleteAlertmanagerConfigAPI() error
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *AlertManagerConfigReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="internal/helpers/mimirtool_helper.go">
package helpers
⋮----
import (
	"context"
	"reflect"

	"github.com/go-logr/logr"
	mimirclient "github.com/grafana/mimir/pkg/mimirtool/client"
	"github.com/grafana/mimir/pkg/mimirtool/rules/rwrulefmt"
	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)
⋮----
"context"
"reflect"
⋮----
"github.com/go-logr/logr"
mimirclient "github.com/grafana/mimir/pkg/mimirtool/client"
"github.com/grafana/mimir/pkg/mimirtool/rules/rwrulefmt"
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
⋮----
const (
	mimirRuleNamespace = "osko"
)
⋮----
type MimirClientConfig struct {
	Address  string
	TenantId string
}
⋮----
func (m *MimirClientConfig) NewMimirClient() (*mimirclient.MimirClient, error)
⋮----
func NewMimirRule(slo *openslov1.SLO, rule *monitoringv1.PrometheusRule, connectionDetails *oskov1alpha1.ConnectionDetails) (mimirRule *oskov1alpha1.MimirRule, err error)
⋮----
func NewMimirRuleGroups(rule *monitoringv1.PrometheusRule, connectionDetails *oskov1alpha1.ConnectionDetails) ([]oskov1alpha1.RuleGroup, error)
⋮----
var ruleGroups []oskov1alpha1.RuleGroup
⋮----
var mimirRules []oskov1alpha1.Rule
⋮----
func GetMimirRuleGroup(log logr.Logger, mimirClient *mimirclient.MimirClient, rule *monitoringv1.PrometheusRule) *rwrulefmt.RuleGroup
⋮----
func UpdateMimirRuleGroup(log logr.Logger, mimirClient *mimirclient.MimirClient, existingGroup *rwrulefmt.RuleGroup, desiredGroup *rwrulefmt.RuleGroup) error
⋮----
func DeleteMimirRuleGroup(log logr.Logger, mimirClient *mimirclient.MimirClient, ruleGroup *rwrulefmt.RuleGroup) error
</file>

<file path="api/osko/v1alpha1/zz_generated.deepcopy.go">
//go:build !ignore_autogenerated
⋮----
// Code generated by controller-gen. DO NOT EDIT.
⋮----
package v1alpha1
⋮----
import (
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	"github.com/prometheus/common/model"
	"k8s.io/apimachinery/pkg/apis/meta/v1"
	runtime "k8s.io/apimachinery/pkg/runtime"
)
⋮----
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
"github.com/prometheus/common/model"
"k8s.io/apimachinery/pkg/apis/meta/v1"
runtime "k8s.io/apimachinery/pkg/runtime"
⋮----
// DeepCopyInto is an autogenerated deepcopy function, copying the receiver, writing into out. in must be non-nil.
func (in *AlertManagerConfig) DeepCopyInto(out *AlertManagerConfig)
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertManagerConfig.
func (in *AlertManagerConfig) DeepCopy() *AlertManagerConfig
⋮----
// DeepCopyObject is an autogenerated deepcopy function, copying the receiver, creating a new runtime.Object.
func (in *AlertManagerConfig) DeepCopyObject() runtime.Object
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertManagerConfigList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertManagerConfigSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new AlertManagerConfigStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new ConnectionDetails.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Cortex.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Mimir.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MimirRule.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MimirRuleList.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MimirRuleSpec.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new MimirRuleStatus.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Multitenancy.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Rule.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new RuleGroup.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new Ruler.
⋮----
// DeepCopy is an autogenerated deepcopy function, copying the receiver, creating a new SecretRef.
</file>

<file path="internal/controller/openslo/slo_controller.go">
package controller
⋮----
import (
	"context"
	"fmt"
	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
	"github.com/oskoperator/osko/internal/helpers"
	"github.com/oskoperator/osko/internal/utils"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/fields"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/record"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
	"sigs.k8s.io/controller-runtime/pkg/handler"
	ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
	"sigs.k8s.io/controller-runtime/pkg/reconcile"
	"time"
)
⋮----
"context"
"fmt"
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
"github.com/oskoperator/osko/internal/helpers"
"github.com/oskoperator/osko/internal/utils"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
apierrors "k8s.io/apimachinery/pkg/api/errors"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
"k8s.io/apimachinery/pkg/fields"
"k8s.io/apimachinery/pkg/runtime"
"k8s.io/apimachinery/pkg/types"
"k8s.io/client-go/tools/record"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
"sigs.k8s.io/controller-runtime/pkg/handler"
ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
"sigs.k8s.io/controller-runtime/pkg/reconcile"
"time"
⋮----
const (
	indicatorRef       = ".spec.indicatorRef"
	errGetSLO          = "could not get SLO Object"
	errDatasourceRef   = "Unable to get Datasource. Check if the referenced datasource exists."
	mimirRuleFinalizer = "finalizer.mimir.osko.dev"
)
⋮----
// SLOReconciler reconciles a SLO object
type SLOReconciler struct {
	client.Client
	Scheme   *runtime.Scheme
	Recorder record.EventRecorder
}
⋮----
//+kubebuilder:rbac:groups=openslo.com,resources=slos,verbs=get;list;watch;create;update;patch;delete
//+kubebuilder:rbac:groups=openslo.com,resources=slos/status,verbs=get;update;patch
//+kubebuilder:rbac:groups=openslo.com,resources=slos/finalizers,verbs=update
//+kubebuilder:rbac:groups=core,resources=events,verbs=create;patch
//+kubebuilder:rbac:groups=monitoring.coreos.com,resources=prometheusrules,verbs=get;list;watch;create;update;patch;delete
⋮----
func (r *SLOReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// Get DS from SLO's ref
⋮----
// Get SLI from SLO's ref
⋮----
func (r *SLOReconciler) createIndices(mgr ctrl.Manager) error
⋮----
func (r *SLOReconciler) findObjectsForSli() func(ctx context.Context, a client.Object) []reconcile.Request
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *SLOReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="go.mod">
module github.com/oskoperator/osko

go 1.23.0

toolchain go1.24.0

require (
	github.com/go-logr/logr v1.4.1
	github.com/grafana/mimir v0.0.0-20231101181902-68d120862184
	github.com/onsi/ginkgo/v2 v2.17.1
	github.com/onsi/gomega v1.32.0
	github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring v0.74.0
	github.com/prometheus/client_golang v1.19.1
	github.com/prometheus/common v0.53.0
	github.com/prometheus/prometheus v1.99.0
	gopkg.in/yaml.v3 v3.0.1
	k8s.io/api v0.30.1
	k8s.io/apimachinery v0.30.1
	k8s.io/client-go v0.30.1
	sigs.k8s.io/controller-runtime v0.18.2
)

require (
	github.com/Azure/azure-sdk-for-go/sdk/azcore v1.11.1 // indirect
	github.com/Azure/azure-sdk-for-go/sdk/azidentity v1.6.0 // indirect
	github.com/Azure/azure-sdk-for-go/sdk/internal v1.8.0 // indirect
	github.com/AzureAD/microsoft-authentication-library-for-go v1.2.2 // indirect
	github.com/DmitriyVTitov/size v1.5.0 // indirect
	github.com/alecthomas/units v0.0.0-20231202071711-9a357b53e9c9 // indirect
	github.com/armon/go-metrics v0.4.1 // indirect
	github.com/aws/aws-sdk-go v1.51.25 // indirect
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/coreos/go-semver v0.3.1 // indirect
	github.com/coreos/go-systemd/v22 v22.5.0 // indirect
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/dennwc/varint v1.0.0 // indirect
	github.com/dgraph-io/ristretto v0.1.1 // indirect
	github.com/dustin/go-humanize v1.0.1 // indirect
	github.com/edsrzf/mmap-go v1.1.0 // indirect
	github.com/efficientgo/core v1.0.0-rc.2 // indirect
	github.com/emicklei/go-restful/v3 v3.12.0 // indirect
	github.com/evanphx/json-patch v5.9.0+incompatible // indirect
	github.com/evanphx/json-patch/v5 v5.9.0 // indirect
	github.com/fatih/color v1.15.0 // indirect
	github.com/fsnotify/fsnotify v1.7.0 // indirect
	github.com/go-kit/log v0.2.1 // indirect
	github.com/go-logfmt/logfmt v0.6.0 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-logr/zapr v1.3.0 // indirect
	github.com/go-openapi/jsonpointer v0.21.0 // indirect
	github.com/go-openapi/jsonreference v0.21.0 // indirect
	github.com/go-openapi/strfmt v0.23.0 // indirect
	github.com/go-openapi/swag v0.23.0 // indirect
	github.com/go-task/slim-sprig v0.0.0-20230315185526-52ccab3ef572 // indirect
	github.com/gogo/googleapis v1.4.1 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/gogo/status v1.1.1 // indirect
	github.com/golang-jwt/jwt/v5 v5.2.2 // indirect
	github.com/golang/glog v1.2.4 // indirect
	github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/golang/snappy v0.0.4 // indirect
	github.com/google/btree v1.0.1 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/pprof v0.0.0-20230926050212-f7f687d19a98 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/grafana/dskit v0.0.0-20231031132813-52f4e8d82d59 // indirect
	github.com/grafana/regexp v0.0.0-20221122212121-6b5c0a4cb7fd // indirect
	github.com/hashicorp/consul/api v1.25.1 // indirect
	github.com/hashicorp/errwrap v1.1.0 // indirect
	github.com/hashicorp/go-cleanhttp v0.5.2 // indirect
	github.com/hashicorp/go-hclog v1.5.0 // indirect
	github.com/hashicorp/go-immutable-radix v1.3.1 // indirect
	github.com/hashicorp/go-msgpack v1.1.5 // indirect
	github.com/hashicorp/go-multierror v1.1.1 // indirect
	github.com/hashicorp/go-rootcerts v1.0.2 // indirect
	github.com/hashicorp/go-sockaddr v1.0.6 // indirect
	github.com/hashicorp/golang-lru v1.0.2 // indirect
	github.com/hashicorp/memberlist v0.5.0 // indirect
	github.com/hashicorp/serf v0.10.1 // indirect
	github.com/imdario/mergo v0.3.16 // indirect
	github.com/jmespath/go-jmespath v0.4.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/jpillora/backoff v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/klauspost/compress v1.17.8 // indirect
	github.com/kylelemons/godebug v1.1.0 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.19 // indirect
	github.com/miekg/dns v1.1.56 // indirect
	github.com/mitchellh/go-homedir v1.1.0 // indirect
	github.com/mitchellh/mapstructure v1.5.0 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/mwitkow/go-conntrack v0.0.0-20190716064945-2f068394615f // indirect
	github.com/oklog/ulid v1.3.1 // indirect
	github.com/opentracing/opentracing-go v1.2.1-0.20220228012449-10b1cf09e00b // indirect
	github.com/pkg/browser v0.0.0-20240102092130-5ac0b6a4141c // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/pmezard/go-difflib v1.0.1-0.20181226105442-5d4384ee4fb2 // indirect
	github.com/prometheus/alertmanager v0.27.0 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common/sigv4 v0.1.0 // indirect
	github.com/prometheus/exporter-toolkit v0.11.0 // indirect
	github.com/prometheus/procfs v0.12.0 // indirect
	github.com/sean-/seed v0.0.0-20170313163322-e2103e2c3529 // indirect
	github.com/sirupsen/logrus v1.9.3 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	github.com/stretchr/testify v1.9.0 // indirect
	github.com/thanos-io/objstore v0.0.0-20231025225615-ff7faac741fb // indirect
	github.com/uber/jaeger-client-go v2.30.0+incompatible // indirect
	github.com/uber/jaeger-lib v2.4.1+incompatible // indirect
	github.com/xlab/treeprint v1.2.0 // indirect
	go.etcd.io/etcd/api/v3 v3.5.10 // indirect
	go.etcd.io/etcd/client/pkg/v3 v3.5.10 // indirect
	go.etcd.io/etcd/client/v3 v3.5.10 // indirect
	go.opentelemetry.io/otel v1.25.0 // indirect
	go.opentelemetry.io/otel/metric v1.25.0 // indirect
	go.opentelemetry.io/otel/trace v1.25.0 // indirect
	go.uber.org/atomic v1.11.0 // indirect
	go.uber.org/goleak v1.3.0 // indirect
	go.uber.org/multierr v1.11.0 // indirect
	go.uber.org/zap v1.26.0 // indirect
	golang.org/x/crypto v0.35.0 // indirect
	golang.org/x/exp v0.0.0-20240325151524-a685a6edb6d8 // indirect
	golang.org/x/mod v0.17.0 // indirect
	golang.org/x/net v0.36.0 // indirect
	golang.org/x/oauth2 v0.19.0 // indirect
	golang.org/x/sync v0.11.0 // indirect
	golang.org/x/sys v0.30.0 // indirect
	golang.org/x/term v0.29.0 // indirect
	golang.org/x/text v0.22.0 // indirect
	golang.org/x/time v0.5.0 // indirect
	golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d // indirect
	gomodules.xyz/jsonpatch/v2 v2.4.0 // indirect
	google.golang.org/genproto v0.0.0-20231002182017-d307bd883b97 // indirect
	google.golang.org/genproto/googleapis/api v0.0.0-20231012201019-e917dd12ba7a // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20231012201019-e917dd12ba7a // indirect
	google.golang.org/grpc v1.59.0 // indirect
	google.golang.org/protobuf v1.34.1 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v2 v2.4.0 // indirect
	k8s.io/apiextensions-apiserver v0.30.1 // indirect
	k8s.io/klog/v2 v2.120.1 // indirect
	k8s.io/kube-openapi v0.0.0-20240322212309-b815d8309940 // indirect
	k8s.io/utils v0.0.0-20240310230437-4693a0247e57 // indirect
	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
	sigs.k8s.io/yaml v1.4.0 // indirect
)

replace github.com/prometheus/prometheus => github.com/grafana/mimir-prometheus v0.0.0-20231101140207-5f9db04c2d53
</file>

<file path="internal/controller/osko/mimirrule_controller.go">
package osko
⋮----
import (
	"context"
	"fmt"
	"reflect"
	"time"

	"github.com/go-logr/logr"
	mimirclient "github.com/grafana/mimir/pkg/mimirtool/client"
	"github.com/grafana/mimir/pkg/mimirtool/rules/rwrulefmt"
	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	"github.com/oskoperator/osko/internal/helpers"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	"github.com/prometheus/common/model"
	"github.com/prometheus/prometheus/model/rulefmt"
	"gopkg.in/yaml.v3"
	apierrors "k8s.io/apimachinery/pkg/api/errors"
	"k8s.io/apimachinery/pkg/types"
	"k8s.io/client-go/tools/record"
	"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
	ctrllog "sigs.k8s.io/controller-runtime/pkg/log"

	oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
	"k8s.io/apimachinery/pkg/runtime"
	ctrl "sigs.k8s.io/controller-runtime"
	"sigs.k8s.io/controller-runtime/pkg/client"
)
⋮----
"context"
"fmt"
"reflect"
"time"
⋮----
"github.com/go-logr/logr"
mimirclient "github.com/grafana/mimir/pkg/mimirtool/client"
"github.com/grafana/mimir/pkg/mimirtool/rules/rwrulefmt"
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
"github.com/oskoperator/osko/internal/helpers"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
"github.com/prometheus/common/model"
"github.com/prometheus/prometheus/model/rulefmt"
"gopkg.in/yaml.v3"
apierrors "k8s.io/apimachinery/pkg/api/errors"
"k8s.io/apimachinery/pkg/types"
"k8s.io/client-go/tools/record"
"sigs.k8s.io/controller-runtime/pkg/controller/controllerutil"
ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
⋮----
oskov1alpha1 "github.com/oskoperator/osko/api/osko/v1alpha1"
"k8s.io/apimachinery/pkg/runtime"
ctrl "sigs.k8s.io/controller-runtime"
"sigs.k8s.io/controller-runtime/pkg/client"
⋮----
// MimirRuleReconciler reconciles a MimirRule object
type MimirRuleReconciler struct {
	client.Client
	Scheme             *runtime.Scheme
	Recorder           record.EventRecorder
	MimirClient        *mimirclient.MimirClient
	RequeueAfterPeriod time.Duration
}
⋮----
const (
	mimirRuleNamespace      = "osko"
	mimirRuleFinalizer      = "finalizer.osko.dev/mimir"
	prometheusRuleFinalizer = "finalizer.osko.dev/prometheusrule"

	errFinalizerAddFailed    = "Failed to add the finalizer to the"
	errFinalizerRemoveFailed = "Failed to remove the finalizer from the"
)
⋮----
// +kubebuilder:rbac:groups=osko.dev,resources=mimirrules,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=osko.dev,resources=mimirrules/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=osko.dev,resources=mimirrules/finalizers,verbs=update
// +kubebuilder:rbac:groups=core,resources=events,verbs=create;patch
⋮----
func (r *MimirRuleReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error)
⋮----
// TODO: This logic is total bullshit. We should revise the reconciliation logic and make it more clear.
⋮----
func (r *MimirRuleReconciler) newMimirClient(connectionDetails *oskov1alpha1.ConnectionDetails) error
⋮----
func (r *MimirRuleReconciler) createMimirRuleGroupAPI(log logr.Logger, rule *oskov1alpha1.RuleGroup) error
⋮----
var mimirRuleNodes []rulefmt.RuleNode
⋮----
var modelDuration model.Duration
⋮----
func (r *MimirRuleReconciler) deleteMimirRuleGroupAPI(log logr.Logger, name string) error
⋮----
// SetupWithManager sets up the controller with the Manager.
func (r *MimirRuleReconciler) SetupWithManager(mgr ctrl.Manager) error
</file>

<file path="helm/osko/Chart.yaml">
apiVersion: v2
name: osko
description: Base Helm chart for OSKO

# A chart can be either an 'application' or a 'library' chart.
#
# Application charts are a collection of templates that can be packaged into versioned archives
# to be deployed.
#
# Library charts provide useful utilities or functions for the chart developer. They're included as
# a dependency of application charts to inject those utilities and functions into the rendering
# pipeline. Library charts do not define any templates and therefore cannot be deployed.
type: application

# This is the chart version. This version number should be incremented each time you make changes
# to the chart and its templates, including the app version.
# Versions are expected to follow Semantic Versioning (https://semver.org/)
version: 0.0.15

# This is the version number of the application being deployed. This version number should be
# incremented each time you make changes to the application. Versions are not expected to
# follow Semantic Versioning. They should reflect the version the application is using.
# It is recommended to use it with quotes.
appVersion: "0.0.15"

home: https://github.com/oskoperator/osko

dependencies:
  - name: prometheus-operator-crds
    version: 8.0.1
    repository: https://prometheus-community.github.io/helm-charts
    condition: installCRDs.prometheus-operator
  - name: crds
    version: 0.0.0
    condition: installCRDs.osko
</file>

<file path="internal/helpers/prometheus_helper.go">
package helpers
⋮----
import (
	"bytes"
	"context"
	"fmt"
	"regexp"
	"sort"
	"strings"
	"text/template"

	openslov1 "github.com/oskoperator/osko/api/openslo/v1"
	"github.com/oskoperator/osko/internal/config"
	monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
	"github.com/prometheus/common/model"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/util/intstr"
	ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
)
⋮----
"bytes"
"context"
"fmt"
"regexp"
"sort"
"strings"
"text/template"
⋮----
openslov1 "github.com/oskoperator/osko/api/openslo/v1"
"github.com/oskoperator/osko/internal/config"
monitoringv1 "github.com/prometheus-operator/prometheus-operator/pkg/apis/monitoring/v1"
"github.com/prometheus/common/model"
metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
"k8s.io/apimachinery/pkg/util/intstr"
ctrllog "sigs.k8s.io/controller-runtime/pkg/log"
⋮----
const (
	RecordPrefix   = "osko"
	promqlTemplate = `
	{{- if eq .RecordName "slo_target" -}}
	vector({{.Metric}})
⋮----
// RuleTemplateData holds data to fill the PromQL template.
type RuleTemplateData struct {
	Metric     string
	Service    string
	Window     string
	Extended   bool
	RecordName string
	Labels     string
}
⋮----
// AlertRuleTemplateData holds data to fill the PromQL template for alerting rules.
type AlertRuleTemplateData struct {
	Metric     string
	Service    string
	Window     string
	RecordName string
	Labels     string
	For        string
}
⋮----
type MonitoringRuleSet struct {
	Slo        *openslov1.SLO
	Sli        *openslov1.SLI
	TargetRule monitoringv1.Rule
	BaseRule   monitoringv1.Rule
	GoodRule   monitoringv1.Rule
	TotalRule  monitoringv1.Rule
	BaseWindow string
}
⋮----
// mapToColonSeparatedString takes a map[string]string and returns a string
// that represents the map's key-value pairs, where each pair is concatenated
// by an equal sign and the pairs are comma-separated.
func mapToColonSeparatedString(labels map[string]string) string
⋮----
// We build the string by iterating over the sorted keys.
⋮----
// Join the key-value pairs with commas and return the result.
⋮----
func mergeLabels(ms ...map[string]string) map[string]string
⋮----
func uniqueStrings(input []string) []string
⋮----
func (mrs *MonitoringRuleSet) createBaseRuleLabels(window string) map[string]string
⋮----
func (mrs *MonitoringRuleSet) createUserDefinedRuleLabels() map[string]string
⋮----
func (mrs *MonitoringRuleSet) createErrorBudgetValueRecordingRule(sliMeasurement monitoringv1.Rule, window string) monitoringv1.Rule
⋮----
func (mrs *MonitoringRuleSet) createErrorBudgetTargetRecordingRule(window string) monitoringv1.Rule
⋮----
func (mrs *MonitoringRuleSet) createSliMeasurementRecordingRule(totalRule, goodRule monitoringv1.Rule, window string) monitoringv1.Rule
⋮----
func (mrs *MonitoringRuleSet) createBurnRateRecordingRule(errorBudgetValue, errorBudgetTarget monitoringv1.Rule, window string) monitoringv1.Rule
⋮----
func (mrs *MonitoringRuleSet) createAntecedentRule(metric, recordName, window string) monitoringv1.Rule
⋮----
// checks if the metric source type of the metric in the SLI is Prometheus-compatible
func (mrs *MonitoringRuleSet) isPrometheusSource() bool
⋮----
func (mrs *MonitoringRuleSet) createRecordingRule(metric, recordName, window string, extended bool) monitoringv1.Rule
⋮----
var promql bytes.Buffer
⋮----
// SetupRules constructs rule groups for monitoring based on SLO and SLI configurations.
func (mrs *MonitoringRuleSet) SetupRules() ([]monitoringv1.RuleGroup, error)
⋮----
baseWindow := mrs.BaseWindow //Should configurable somewhere as agreed on product workshop
⋮----
extendedWindow := "28d" //Default to 28d if not specified in the SLO
⋮----
var rules = map[string]map[string]monitoringv1.Rule{
		"targetRule":        {},
		"totalRule":         {},
		"goodRule":          {},
		"badRule":           {},
		"sliMeasurement":    {},
		"errorBudgetValue":  {},
		"errorBudgetTarget": {},
		"burnRate":          {},
	}
⋮----
var alertRuleErrorBudgets []monitoringv1.Rule
⋮----
// BASE WINDOW
⋮----
// TODO: having the magic alerting we still need to figure out how to pass the severity into the alerting rule when we have more than one alerting tool.
// The idea is to have a map of the alerting tool and the severity and then iterate over all connected AlertNotificationTargets.
⋮----
var alertRules []monitoringv1.Rule
⋮----
config.PageCritical, // Fast burn rate, short window
⋮----
config.PageHigh, // Fast burn rate, long window
⋮----
config.TicketHigh, // Slow burn rate, short window
⋮----
config.TicketMedium, // Slow burn rate, long window
⋮----
// log.V(1).Info("Alerting rules to be created", "alertRules", alertRules)
⋮----
// createMagicMultiBurnRateAlert creates a Prometheus alert rule for multi-burn rate alerting.
func (mrs *MonitoringRuleSet) createMagicMultiBurnRateAlert(burnRates []monitoringv1.Rule, threshold string, duration *monitoringv1.Duration, sreSeverity config.SREAlertSeverity) monitoringv1.Rule
⋮----
// Populate the alertingPageWindows map with the actual burn rates
⋮----
//TODO: Create severity mapping between alerting tool and SRE book severity
⋮----
// Define the alert expressions for different severities and durations
var alertExpression string
⋮----
func CreateAlertingRule() (*monitoringv1.PrometheusRule, error)
⋮----
func CreatePrometheusRule(slo *openslov1.SLO, sli *openslov1.SLI) (*monitoringv1.PrometheusRule, error)
⋮----
// log.V(1).Error(err, "Failed to create the PrometheusRule")
</file>

</files>
